slug: nomad-monitoring
id: mxfbvvo8n5l2
type: track
title: Nomad Monitoring
teaser: |
  Monitor a Nomad cluster and jobs with Prometheus.
description: |-
  The Nomad client and server agents collect runtime [telemetry](https://www.nomadproject.io/docs/telemetry/index.html). Operators can use this data to gain real-time visibility into their Nomad clusters and improve performance. Additionally, Nomad operators can set up monitoring and alerting against these metrics and export the metrics to tools like Prometheus, Grafana, Graphite, DataDog, and Circonus.

  This track will guide you through implementing the [Using Prometheus to Monitor Nomad Metrics](https://learn.hashicorp.com/tutorials/nomad/prometheus-metrics) guide.

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics), [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster), and the [Nomad Multi-Server Cluster](https://instruqt.com/hashicorp/tracks/nomad-multi-server-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- Nomad
- monitoring
- telemetry
- Prometheus
owner: hashicorp
developers:
- roger@hashicorp.com
- pphan@hashicorp.com
private: false
published: true
show_timer: true
challenges:
- slug: fabio-and-prometheus-jobs
  id: m4ly1zb0xyjh
  type: challenge
  title: Deploy Fabio and Prometheus
  teaser: |
    Run Nomad jobs that deploy Fabio and Prometheus to the Nomad cluster.
  assignment: |-
    In this challenge, you will run jobs that deploy Fabio and Prometheus to the Nomad cluster.

    That's right: We'll be monitoring Nomad with an application (Prometheus) deployed by Nomad to the Nomad cluster!

    First check that the Nomad and Consul agents were deployed properly by running these commands on the "Server" tab:
    ```
    consul members
    nomad server members
    nomad node status
    ```

    The first should show 4 Consul agents, the second should show 1 Nomad server, and the third should show 3 Nomad clients.

    ## Run the Fabio Job
    The job file, `fabio.nomad`, for Fabio has been created for you. Please inspect it on the "Config" tab and note the following:
      * It defines the system job `fabio`.
      * It includes one task group, `fabio`.
      * That task group includes a single task, `fabio` that uses the Docker task driver to run the image `fabiolb/fabio`.

    Please run the Fabio job on the "Server" tab.
    ```
    cd nomad
    nomad job run fabio.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "69d46d05"
    Evaluation triggered by job "fabio"
    Allocation "119ebcfe" created: node "ce92fbb1", group "fabio"
    Allocation "4fae262f" created: node "3800d575", group "fabio"
    Allocation "4febc9ef" created: node "da58f06c", group "fabio"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "69d46d05" finished with status "complete"
    `

    Verify that the `fabio` job is running on 3 nodes:
    ```
    nomad job status fabio | grep -A 5 Allocations
    ```
    This should return something like this:<br>
    `
    Allocations
    ID        Node ID   Task Group  Version  Desired  Status   Created     Modified
    119ebcfe  ce92fbb1  fabio       0        run      running  19m23s ago  19m20s ago
    4fae262f  3800d575  fabio       0        run      running  19m23s ago  19m20s ago
    4febc9ef  da58f06c  fabio       0        run      running  19m23s ago  19m20s ago
    `<br>
    If you would like to see the full job status, you can leave off the `grep` command.

    ## Visit the Fabio UI
    * Click on the "Fabio" tab. This connects to the first Nomad client on port `9998` and shows the fabio UI.
    * The Fabio UI is available on all the Nomad clients, but we only exposed a tab for the first one.
    * The Fabio routing table will be empty. We have not deployed anything that fabio can route to yet.

    ## Run the Prometheus Job
    The setup script for the server has deployed two versions of a job specification file that runs Prometheus, `prometheus1.nomad` and `prometheus2.nomad`, both of which specify the job name as `prometheus`. You'll use the first file in this challenge and use the second one in the next two challenges.

    Please inspect the `prometheus1.nomad` job specification file on the "Config" tab now and note the following:
      * It defines the job `prometheus`.
      * It includes one task group, `monitoring`.
      * That task group includes a single task, `prometheus`.
      * That task uses the Docker task driver to run the image `prom/prometheus:latest` and uses a template to write out a file called `local/prometheus.yml` to the client that ends up running the task group.
      * It also registers the task as a service called `prometheus` with Consul.

    Please run the `prometheus1.nomad` job now on the "Server" tab:
    ```
    nomad job run prometheus1.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "bdef8e9d"
    Evaluation triggered by job "prometheus"
    Evaluation within deployment: "4d2a259d"
    Allocation "02ef03c9" created: node "da58f06c", group "monitoring"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "bdef8e9d" finished with status "complete"
    `

    Verify that the `prometheus` job is running on 1 node:
    ```
    nomad job status prometheus | grep -A 5 Allocations
    ```
    This should return something like this:<br>
    `
    Allocations
    ID        Node ID   Task Group  Version  Desired  Status   Created     Modified
    02ef03c9  da58f06c  monitoring  0        run      running  12m54s ago  12m34s ago
    `<br>
    If you would like to see the full job status, you can leave off the `grep` command.

    ## Visit the Prometheus UI
    * Connect to the Prometheus UI on any of the Nomad clients (using tabs "Prometheus1", "Prometheus2", or "Prometheus3").
    * There is only one instance of Prometheus running, but fabio routes you to the correct node.
    * You might need to click the refresh icon (top right).


    Use Prometheus to query how many jobs are running on your Nomad cluster in the Prometheus UI.
    * Type `nomad_nomad_job_summary_running` into the query field and press your <return\> key.
    * Click the "Execute" button.
    * Note that you can click the rectangular icon next to the refresh icon to hide and redisplay the assignment in order to see the results more easily.
    * The value of the `fabio` job is `3` since it is using the [system](https://www.nomadproject.io/docs/schedulers.html#system) scheduler type and we are running three Nomad clients in our demo cluster.
    * The value of our `prometheus` job is `1` since we only deployed one instance of it.
    * To see what this looks like, visit this [page](https://learn.hashicorp.com/img/nomad/operating-nomad/running-jobs.png).
    * To see the description of other metrics, visit the [telemetry](https://www.nomadproject.io/docs/configuration/telemetry.html) section of the Nomad documentation.

    If you go back to the "Fabio" tab and click the Instruqt refresh icon, you will now see the "prometheus" service in the routing table.  (Note that you cannot visit it using the URL in the "Dest" column because Instruqt will not allow access to the Prometheus UI on the "Fabio" tab.)

    In the next challenge, you will deploy the Prometheus Alertmanager to the Nomad cluster and modify the `prometheus` job to integrate the Prometheus server with Alertmanager.
  notes:
  - type: text
    contents: |-
      The Nomad client and server agents collect runtime [telemetry](https://www.nomadproject.io/docs/telemetry/index.html). Operators can use this data to gain real-time visibility into their Nomad clusters and improve performance.

      Additionally, Nomad operators can set up monitoring and alerting against these metrics and export the metrics to tools like Prometheus, Grafana, Graphite, DataDog, and Circonus.

      This track will guide you through implementing the [Using Prometheus to Monitor Nomad Metrics](https://learn.hashicorp.com/tutorials/nomad/prometheus-metrics) guide.
  - type: text
    contents: |-
      The setup scripts for this track have configured 4 VMs running Nomad and Consul agents. One is functioning as a server while the other 3 are functioning as clients, both with regard to Nomad and to Consul.

      We've put all the server and client configuration files and job specification files on the nomad-server VM so that you can conveniently see them in a single Instruqt tab.
  - type: text
    contents: |-
      In this challenge, you will first run a job that deploys the reverse proxy server [Fabio](https://fabiolb.net) that uses [Consul](https://www.consul.io).

      You will then run a job that deploys [Prometheus](https://prometheus.io/docs/introduction/overview), an open-source systems monitoring and alerting tool.
  tabs:
  - title: Config
    type: code
    hostname: nomad-server
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server
  - title: Nomad UI
    type: service
    hostname: nomad-server
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Prometheus1
    type: service
    hostname: nomad-client-1
    port: 9999
  - title: Prometheus2
    type: service
    hostname: nomad-client-2
    port: 9999
  - title: Prometheus3
    type: service
    hostname: nomad-client-3
    port: 9999
  difficulty: basic
  timelimit: 1200
- slug: add-alertmanager
  id: aaqomh65hrcz
  type: challenge
  title: Deploy Alertmanager
  teaser: |
    Run a Nomad job that deploys Alertmanager to the Nomad cluster.
  assignment: |-
    In this challenge, you will deploy the Prometheus Alertmanager to the Nomad cluster and modify the `prometheus` job to integrate the Prometheus server with Alertmanager.

    NOTE: Prometheus sends alerts to Alertmanager. Alertmanager sends out notifications to designated receivers.

    We have added a tab for the Consul UI for those who would like to see the services registered by Nomad for some of the tasks it starts.

    ## Run the Alertmanager Job
    The `alertmanager` job has already been created for you. Please view the file called `alertmanager.nomad` on the  "Config" tab and note the following:
      * It defines the job `alertmanager`.
      * It includes one task group, `alerting`.
      * That task group includes a single task, `alertmanager`.
      * That task uses the Docker task driver to run the image `prom/alertmanager:latest`.
      * It also registers the task as a service called `alertmanager` with Consul.

    Next, run the `alertmanager` job on the "Server" tab:
    ```
    cd nomad
    nomad job run alertmanager.nomad
    ```
    You should see something like this:<br>
    `
    ==> Monitoring evaluation "a988ec2e"
    Evaluation triggered by job "alertmanager"
    Evaluation within deployment: "feb7f088"
    Allocation "bd0b5024" created: node "72635e92", group "alerting"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "a988ec2e" finished with status "complete"
    `

    Check the status of the `alertmanager` job on the "Server" tab:
    ```
    nomad job status alertmanager | grep -A 5 Allocations
    ```
    You should see something like this:<br>
    `
    Allocations
    ID        Node ID   Task Group  Version  Desired  Status   Created   Modified
    bd0b5024  72635e92  alerting    0        run      running  3m8s ago  2m51s ago
    `<br>
    If you would like to see the full job status, you can leave off the `grep` command.

    ## Modify and Re-run the Prometheus Job
    Please view the job specification file called `prometheus2.nomad` on the "Config" tab. We added a few important sections to this job file that were not in `prometheus1.nomad`:
    * We added another template stanza that defines an alerting rule for our web server. Prometheus will send out an alert if it detects the `webserver` service has disappeared. This writes the file `local/webserver_alert.yml` to the client that ends up running the task group.
    * We added an `alerting` block as well as a `rule_files` block to make Prometheus aware of Alertmanager and the rule we have defined to our Prometheus configuration file, `local/prometheus.yml`.
    * We have configured Prometheus to scrape Alertmanager and our web server in addition to the Nomad metrics that it was already scraping.

    Please run the modified `prometheus` job on the "Server" tab.
    ```
    nomad job run prometheus2.nomad
    ```
    You should see output similar to what you saw when you ran other jobs.

    Next, check the status of the `prometheus` job
    ```
    nomad job status prometheus | grep -A 5 Allocations
    ```
    You should see one `running` allocation and one `complete` allocation. If you would like to see the full job status, you can leave off the `grep` command.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy the Prometheus [Alertmanager](https://prometheus.io/docs/alerting/alertmanager) to the Nomad cluster.

      You will then modify the `prometheus` job to integrate the Prometheus server with Alertmanager and redeploy Prometheus.
  tabs:
  - title: Config
    type: code
    hostname: nomad-server
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server
  - title: Consul UI
    type: service
    hostname: nomad-server
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Prometheus1
    type: service
    hostname: nomad-client-1
    port: 9999
  difficulty: basic
  timelimit: 1200
- slug: add-web-server
  id: wzwlz6v21qqr
  type: challenge
  title: Run and Stop a Web Server and See Alerts
  teaser: |
    Deploy a web server to the Nomad cluster, stop it, and see alerts in Prometheus and Alertmanager.
  assignment: |-
    In this challenge, you will deploy a web server to the Nomad cluster by running the `webserver.nomad` job. You will then stop and restart it to see alerts show up and be cleared in Prometheus.

    ## Run the Web Server Job
    The `webserver` job has already been created for you. Please view the file `webserver.nomad` on the "Config" tab and note the following:
      * It defines the job `webserver`.
      * It includes one task group, `webserver`.
      * That task group includes a single task, `server` that uses the Docker task driver to run the image `hashicorp/demo-prometheus-instrumentation:latest`.
      * It also registers the task as a service called `webserver` with Consul.

    Next, run the `webserver` job on the "Server" tab:
    ```
    cd nomad
    nomad job run webserver.nomad
    ```
    This will return results similar to what you've seen when running other jobs.

    Now, check the status of the `webserver` job on the "Server" tab:
    ```
    nomad job status webserver | grep -A 5 Allocations
    ```
    This should return something like this:<br>
    `
    Allocations
    ID        Node ID   Task Group  Version  Desired  Status   Created  Modified
    ffbc5b06  72635e92  webserver   0        run      running  1m ago   45s ago
    `<br>
    If you would like to see the full job status, you can leave off the `grep` command.

    ## Inspect Prometheus Targets and Alerts
    Please do the following steps on the "Prometheus" tab. (Note that we are now only showing one tab for it.)
    * Select Status -> Targets.
    * You will see `webserver` and `alertmanager` appear in your list of targets.
    * To see what this looks like, visit this [page](https://learn.hashicorp.com/img/nomad/operating-nomad/new-targets.png).
    * Select the "Alerts" tab of the Prometheus UI.
    * Note the alert that we configured for the web server. No alerts are active because our web server is running.
    * To see what this looks like, visit this [page](https://learn.hashicorp.com/img/nomad/operating-nomad/alerts.png).

    ## Stop the Web Server to See Alerts
    Next, please stop your webserver on the "Server" tab:
    ```
    nomad job stop webserver
    ```

    Now, let's see what Prometheus is showing on the "Prometheus" tab.
    * Select the "Alerts" tab of the Prometheus UI (even if it is already selected).
    * Note that we now have an active "Webserver down" alert.
    * To see what this looks like, visit this [page](https://learn.hashicorp.com/img/nomad/operating-nomad/active-alert.png).

    Verify that Alertmanager has received this alert as well on the "Alertmanager" tab.
    * You should see that Alertmanager has received the alert and is showing it as `alertname="Webserver down"`. (You will probably want to click the rectangular Instruqt icon to temporarily hide this assignment to get the Alertmananger spread out horizontally more.)
    * To see what this looks like, visit this [page](https://learn.hashicorp.com/img/nomad/operating-nomad/alertmanager-webui.png).

    ## Restart the Web Server
    Please restart the `webserver` job on the "Server" tab:
    ```
    nomad job run webserver.nomad
    ```

    Then, go to the "Prometheus" tab and click the "Alerts" menu (even if it is already selected). Note that the "Webserver down" alert is no longer active. (You might have to click the "Aerts" menu a few times.)

    Finally, go to the "Alertmanager" tab, click the "Alerts" tab, and verify that Alertmanager has removed the "Webserver down" alert.

    Congratulations on completing the Nomad Monitoring track. Well done!
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy a web server to the Nomad cluster.

      You will then stop the web server and see alerts generated in Prometheus and Alertmanager from the metrics that Nomad is sending them.

      You will also restart the web server and verify that the alerts are no longer active.
  tabs:
  - title: Config
    type: code
    hostname: nomad-server
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server
  - title: Consul UI
    type: service
    hostname: nomad-server
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Prometheus
    type: service
    hostname: nomad-client-1
    port: 9999
  - title: Alertmanager
    type: service
    hostname: nomad-client-1
    path: /alertmanager
    port: 9999
  difficulty: basic
  timelimit: 1200
checksum: "4598853773109752482"
