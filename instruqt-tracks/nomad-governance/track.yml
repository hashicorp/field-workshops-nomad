slug: nomad-governance
id: 05ovf6ut5ojh
type: track
title: Nomad Enterprise Governance
teaser: |
  Learn how to use Nomad Enterprise's governance capabilities including namespaces, resource quotas, and Sentinel policies.
description: |-
  This track will show you how to use Nomad Enterprise's governance capabilities including [Audit Logging](https://www.nomadproject.io/docs/enterprise#audit-logging), [Resource Quotas](https://learn.hashicorp.com/tutorials/nomad/quotas), [Sentinel Policies](https://learn.hashicorp.com/tutorials/nomad/sentinel), and [Cross-Namespace Queries](https://www.nomadproject.io/docs/enterprise#cross-namespace-queries).

  You will also configure and use [Namespaces](https://learn.hashicorp.com/tutorials/nomad/namespaces) and [Nomad ACLs](https://learn.hashicorp.com/tutorials/nomad/access-control). Namespaces are required when using resource quotas. ACLs are applicable to namespaces and resource quotas and are required by Nomad Sentinel policies.

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics) and [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- nomad
- governance
- namespaces
- resource quotas
- Sentinel
- ACLs
owner: hashicorp
developers:
- roger@hashicorp.com
private: true
published: true
show_timer: true
challenges:
- slug: verify-nomad-cluster-health
  id: ktmjptuistsh
  type: challenge
  title: Verify the Health of Your Nomad Enterprise Cluster
  teaser: |
    Verify the health of the Nomad Enterprise cluster that has been deployed for you.
  notes:
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

      In later challenges, you will enable Nomad Enterprise's [Audit Logging](https://www.nomadproject.io/docs/enterprise#audit-logging) and define Nomad [Namespaces](https://learn.hashicorp.com/tutorials/nomad/namespaces), [Resource Quotas](https://learn.hashicorp.com/tutorials/nomad/quotas), [Nomad ACLs](https://learn.hashicorp.com/tutorials/nomad/access-control), and [Sentinel Policies](https://learn.hashicorp.com/tutorials/nomad/sentinel).

      You will then run jobs and learn how Sentinel policies and resource quotas restrict them. You'll also run a [Cross-Namespace Query](https://www.nomadproject.io/docs/enterprise#cross-namespace-queries)
  assignment: |-
    In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients. Each of these has 3.75GB of memory and 1 virtual CPU with 2,300MHz of CPU capacity. This means that the total capacity of the 3 Nomad clients is 11.25GB of memory and 6,900MHz of CPU capacity.

    The cluster is using Nomad 1.0.0 and Consul 1.9.0.

    First, verify that all 4 Consul agents are running and connected to the cluster by running this command on the "Server" tab:
    ```
    consul members
    ```
    You should see 4 Consul agents.

    Check that the Nomad server is running by running this command on the "Server" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server.

    Check the status of the Nomad client nodes by running this command on the "Server" tab:
    ```
    nomad node status
    ```
    You should see 3 Nomad clients.

    In the next challenge, you will enable Nomad Enterprise's audit logging.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Client 3
    type: terminal
    hostname: nomad-client-3
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 300
- slug: nomad-auditing
  id: 6fiot8pxrty5
  type: challenge
  title: Enable Nomad Enterprise Audit Logging
  teaser: |
    Learn how to enable Nomad Enterprise audit logging.
  notes:
  - type: text
    contents: |-
      In this challenge, you will enable Nomad Enterprise's [Audit Logging](https://www.nomadproject.io/docs/enterprise#audit-logging) on your server and clients, giving Nomad operators complete records about all user-issued actions in Nomad.

      With Nomad audit logging, enterprises can proactively identify access anomalies, ensure enforcement of their security policies, and diagnose cluster behavior by viewing preceding user operations.

      In the next challenge, you will configure Nomad namespaces and resource quotas.
  assignment: |-
    In this challenge, you will enable Nomad Enterprise's audit logging on your server and clients, giving Nomad operators complete records about all user-issued actions in Nomad.

    ## Enable Audit Loggiong
    Start by reviewing the "nomad-server1.hcl", "nomad-client1.hcl", "nomad-client1.hcl", and "nomad-client1.hcl" files on the "Config Files" tab. Notice that the following stanza has been added to the bottom of each file:
    ```
    audit {
      enabled = true
    }
    ```

    This stanza is needed on Nomad servers and clients when enabling Nomad Enterprise audit logging. You are looking at copies of the actual files that have been placed in the /etc/nomad.d directory on the server and all 3 clients.

    While the above stanza is very short, it actually does more than you might think since Nomad uses the following defaults for audit logging:
    ```
    audit {
      enable = true
      sink "audit" {
        type               = "file"
        delivery_guarantee = "enforced"
        format             = "json"
        path               = "/[data_dir]/audit/audit.log"
      }
    }
    ```
    where `[data_dir]` is the data directory specified in the Nomad agent configuration.

    In our case, that directory is "/tmp/nomad/server1" for the server, "/tmp/nomad/client1" for nomad-client-1, "/tmp/nomad/client2" for nomad-client-2, and "/tmp/nomad/client3" for nomad-client-3. The audit log for each agent (server or client) is only accessible at this time from the file system on the agent's host.

    It is also possible to configure various properties of the sink including:
      * Whether delivery is guaranteed or not.
      * The path of the audit log.
      * Rotation settings.

    Note that it is also possible to configure filters to limit which events are actually capture in the audit logs. For instance, you might be less concerned about read operations than write operations.

    Restart the Nomad server on the "Server" tab with this command:
    ```
    systemctl restart nomad
    ```

    After waiting 15 seconds, you should restart all 3 of the Nomad clients on the "Client 1", "Client 2", and "Client 3" tabs:
    ```
    systemctl restart nomad
    ```

    ## Inspect Audit Logs
    Now, you will invoke a few Nomand CLI commands and inspect the associated audit log entries.

    Run the following Nomad command to list running jobs on the "Server" tab:
    ```
    nomad job status
    ```

    Now, run the following command to inspect the end of the audit log on the server:
    ```
    cat /tmp/nomad/server1/audit/audit.log | jq .
    ```
    You should see the following OperationReceived log entry:
    ```
    {
      "created_at": <date>,
      "event_type": "audit",
      "payload": {
        "id": <id>,
        "stage": "OperationReceived",
        "type": "audit",
        "timestamp": <timestamp>,
        "version": 1,
        "auth": {
          ...
        },
        "request": {
          "id": <id>,
          "operation": "GET",
          "endpoint": "/v1/jobs",
          "namespace": {
            "id": "default"
          },
          "request_meta": {
            ...
          },
          "node_meta": {
            "ip": "10.132.0.33:4646"
          }
        }
      }
    }
    ```
    followed by the OperationComplete log entry:<br>
    ```
    {
      "created_at": <date>,
      "event_type": "audit",
      "payload": {
        "id": <id>,
        "stage": "OperationComplete",
        "type": "audit",
        "timestamp": <timestamp>,
        "version": 1,
        "auth": {
          ...
        },
        "request": {
          "id": <id>,
          "operation": "GET",
          "endpoint": "/v1/jobs",
          "namespace": {
            "id": "default"
          },
          "request_meta": {
            "remote_address": "127.0.0.1:38742",
            "user_agent": "Go-http-client/1.1"
          },
          "node_meta": {
            "ip": "10.132.0.33:4646"
          }
        },
        "response": {
          "status_code": 200
        }
      }
    }
    ```

    We have deleted some of the data for formatting reasons.

    Next, run `nomad node status` on the "Client 1" tab and then examine the audit log on that tab:
    ```
    cat /tmp/nomad/client1/audit/audit.log | jq .
    ```
    You will again see OperationReceived and OperationComplete log entries.

    Feel free throughout this track to inspect the audit logs on the server or any of the clients after running any Nomad commands on them. Or just inspect the audit logs after completing the track.

    In the next challenge, you will configure Nomad namespaces and resource quotas.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Client 3
    type: terminal
    hostname: nomad-client-3
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 300
- slug: namespaces-and-resource-quotas
  id: edtenqca7eaj
  type: challenge
  title: Configure Nomad Namespaces and Resource Quotas
  teaser: |
    Configure Default, Dev, and QA Nomad namespaces and resource quotas
  notes:
  - type: text
    contents: |-
      In this challenge, you will create "default", "dev", and "qa" resource quotas.

      You will then apply the "default" resource quota to the pre-existing "default" namespace, and create the "dev" and "qa" namespaces. You will apply the "dev" and "qa" resource quotas to the corresponding namespaces when creating them.
  assignment: |-
    In this challenge, you will configure Default, Dev, and QA Nomad namespaces and resource quotas.

    Start by inspecting the "quota-default.hcl", "quota-dev.hcl", and "quota-qa.hcl" resource quota specifications in the quotas directory on the "Config Files" tab. These are written in the HashiCorp Configuration Language (HCL). The first sets global limits of 2,300 MHz of CPU capacity and 3,100 MB of memory while the others set global limits of 2,300 MHz of CPU capacity and 4,100 MB of memory.

    The resource quota specification files are all in the /root/nomad/quotas directory, so please navigate to it with this command:
    ```
    cd /root/nomad/quotas
    ```

    Next, create the "default" resource quota by running this command on the "Server" tab:
    ```
    nomad quota apply quota-default.hcl
    ```
    This should return 'Successfully applied quota specification "default"!'.

    Now create the "dev" resource quota by running this command on the "Server" tab:
    ```
    nomad quota apply quota-dev.hcl
    ```
    This should return 'Successfully applied quota specification "dev"!'.

    Then create the "qa" resource quota by running this command on the "Server" tab:
    ```
    nomad quota apply quota-qa.hcl
    ```
    This should return 'Successfully applied quota specification "qa"!'.

    Now that you've created the resource quotas, you can apply the "default" resource quota to the pre-existing "default" namespace and create the "dev" and "qa" namespaces. You will apply the "dev" and "qa" resource quotas to the corresponding namespaces when creating them.

    Apply the "default" resource quota to the "default" namespace with this command on the "Server" tab:
    ```
    nomad namespace apply -quota default -description "default namespace" default
    ```
    This should return 'Successfully applied namespace "default"!'.

    Create the "dev" namespace and apply the "dev" resource quota to it with this command:
    ```
    nomad namespace apply -quota dev -description "dev namespace" dev
    ```
    This should return 'Successfully applied namespace "dev"!'.

    Create the "qa" namespace and apply the "qa" resource quota to it with this command:
    ```
    nomad namespace apply -quota qa -description "qa namespace" qa
    ```
    This should return 'Successfully applied namespace "qa"!'.

    To become familiar with the Nomad CLI commands for namespaces and resource quotas, we suggest you run the following commands on the "Server" tab:
    ```
    nomad namespace list
    nomad namespace status dev
    nomad quota list
    nomad quota inspect qa
    ```

    The first of these should show the "default", "dev", and "qa" namespaces.

    The second command should show this:<br>
    `
    Name        = dev
    Description = dev namespace
    Quota       = dev
    Quota Limits
    Region  CPU Usage  Memory Usage  Network Usage
    global  0 / 4600   0 / 4100      - / inf
    `<br>

    The third command should show the "default", "dev", and "qa" resource quotas.

    The last command should show a JSON document with information about the resource quota's limits and current usage. There is also a `nomad quota status` command that gives back streamlined information more like what the `nomad namespace status` command gives.

    In the next challenge, you will enable Nomad's ACL system and define ACL policies and tokens which are applicable to Nomad namespaces and resource quotas and required by Nomad Sentinel policies.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 600
- slug: nomad-acls
  id: lvcxijjtmsoy
  type: challenge
  title: Create Nomad ACL Policies and Tokens
  teaser: |
    Enable Nomad's ACL system and create ACL policies and tokens for the Dev and QA teams.
  notes:
  - type: text
    contents: |-
      In this challenge, you will enable Nomad's ACL system and define ACL policies and tokens for the Dev and QA teams that will be using the "dev" and "qa" namespaces.

      Nomad ACLs are also required by Nomad Sentinel policies.
  assignment: |-
    In this challenge, you will enable Nomad's ACL system and define ACL policies and tokens for the Dev and QA teams that will be using the "dev" and "qa" namespaces.

    Nomad ACLs are also required to use Nomad Sentinel policies since only users with the suitable ACL policies can apply Sentinel policies.

    ## Enable the ACL System
    Start by reviewing the "nomad-server1.hcl", "nomad-client1.hcl", "nomad-client2.hcl", and "nomad-client3.hcl" files on the "Config Files" tab. Notice that the following stanza has been added to the bottom of each file:<br>
    `
    acl {
      enabled = true
    }
    `<br>

    This stanza is needed on Nomad servers and clients when enabling the Nomad ACL system. You are looking at copies of the actual files that have been placed in the /etc/nomad.d directory on the server and all 3 clients.

    Navigate to the /root/nomad/acls directory on the "Server" tab with this command:
    ```
    cd /root/nomad/acls
    ```

    Restart the Nomad server on the "Server" tab with this command:
    ```
    systemctl restart nomad
    ```

    After waiting about 15 seconds, bootstrap the Nomad ACLs system by running this command on the "Server" tab:
    ```
    nomad acl bootstrap | tee bootstrap.txt
    ```
    This will return something like this:<br>
    `
    Accessor ID  = 5b7fd453-d3f7-6814-81dc-fcfe6daedea5
    Secret ID    = 9184ec35-65d4-9258-61e3-0c066d0a45c5
    Name         = Bootstrap Token
    Type         = management
    Global       = true
    Policies     = n/a
    Create Time  = 2020-03-09 14:20:57.209983275 +0000 UTC
    Create Index = 68
    Modify Index = 68
    `<br>
    The actual Nomad boostrap token is the value of the "Secret ID" field.

    Export the bootstrap token with these commands on the "Server" tab:
    ```
    export NOMAD_TOKEN=$(cat bootstrap.txt | grep Secret | cut -d' ' -f7)
    echo "export NOMAD_TOKEN=$NOMAD_TOKEN" >> /root/.bash_profile
    ```
    The second command ensures that the bootstrap token will be exported in later challenges.

    Verify that you can still use the Nomad CLI by running `nomad status` which should return "No running jobs".

    Next, you should restart all 3 of the Nomad clients on the "Client 1", "Client 2", and "Client 3" tabs:
    ```
    systemctl restart nomad
    ```

    ## Create ACL Policies and Tokens
    Next, inspect the "acl-anonymous.hcl", "acl-dev.hcl", "acl-qa.hcl", and "acl-override.hcl" Nomad ACL policies in the "acls" directory on the "Config Files" tab. The first allows any user to list jobs in the "default" namespace and read information about agents and nodes. The "dev" and "qa" policies give the same permissions but add the ability to write to the "dev" and "qa" namespaces respectively. The "acl-override.hcl" Nomad ACL allows a user to submit jobs and override soft-mandatory failures of Sentinel policies.

    Add the "anoymous" ACL policy to your Nomad cluster by running this command on the "Server" tab:
    ```
    nomad acl policy apply -description "restricted access for users without ACL tokens" anonymous acl-anonymous.hcl
    ```
    This should return 'Successfully wrote "anonymous" ACL policy!'.

    Add the "dev" ACL policy to your Nomad cluster by running this command on the "Server" tab:
    ```
    nomad acl policy apply -description "access for users with dev ACL tokens" dev acl-dev.hcl
    ```
    This should return 'Successfully wrote "dev" ACL policy!'.

    Add the "qa" ACL policy to your Nomad cluster by running this command on the "Server" tab:
    ```
    nomad acl policy apply -description "access for users with qa ACL tokens" qa acl-qa.hcl
    ```
    This should return 'Successfully wrote "qa" ACL policy!'.

    Add the "override" ACL policy to your Nomad cluster by running this command on the "Server" tab:
    ```
    nomad acl policy apply -description "override soft-mandatory Sentinel policies" override acl-override.hcl
    ```
    This should return 'Successfully wrote "override" ACL policy!'.

    Complete this challenge by adding ACL tokens for use by a developer, Alice, a QA engineer, Bob, and an infrastructure manager, Charlie.

    Create Alice's token and assign the "dev" policy to it with this command on the "Server" tab:
    ```
    nomad acl token create -name="alice" -policy=dev | tee alice-token.txt
    ```
    This will return something like this:<br>
    `
    Accessor ID  = <accessor id>
    Secret ID    = <token>
    Name         = alice
    Type         = client
    Global       = false
    Policies     = [dev]
    Create Time  = <timestamp>
    Create Index = 106
    Modify Index = 106
    `<br>

    We had you use the `tee` command to write Alice's token to a file for later use.

    Create Bob's token and assign the "qa" policy to it with this command on the "Server" tab:
    ```
    nomad acl token create -name="bob" -policy=qa | tee bob-token.txt
    ```
    This will return something like this:<br>
    `
    Accessor ID  = <accessor id>
    Secret ID    = <token>
    Name         = bob
    Type         = client
    Global       = false
    Policies     = [qa]
    Create Time  = <timestamp>
    Create Index = 109
    Modify Index = 109
    `<br>

    We had you use the `tee` command to write Bob's token to a file for later use.

    Create Charlie's token and assign the "override" policy to it with this command on the "Server" tab:
    ```
    nomad acl token create -name="charlie" -policy=override | tee charlie-token.txt
    ```
    This will return something like this:<br>
    `
    Accessor ID  = <accessor id>
    Secret ID    = <token>
    Name         = charlie
    Type         = client
    Global       = false
    Policies     = [override]
    Create Time  = <timestamp>
    Create Index = 109
    Modify Index = 109
    `<br>

    We had you use the `tee` command to write Charlie's token to a file for later use.

    Each user's ACL token is the value of the "Secret ID" field returned by the above commands.

    To become familiar with the Nomad CLI commands for ACLs, we suggest you run the following commands on the "Server" tab:
    ```
    nomad acl policy list
    nomad acl policy info dev
    nomad acl token list
    ```

    The first of these should show the "anonymous", "dev", "qa", and "override" policies.

    The second should show the name, description, and actual rules for the "dev" Sentinel policy.

    The third should show the "Bootstrap", "alice", "bob", and "charlie" tokens.

    We also suggest you run this command:<br>
    `
    nomad acl token info <accessor_id>
    `<br>
    replacing <accessor_id\> with Alice's or Bob's Accessor ID. You will see complete information for their token including the actual token itself (the "Secret ID" field).

    In the next challenge, you will define and enable some Sentinel policies to place restrictions on jobs that can be run in your Nomad cluster.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Client 3
    type: terminal
    hostname: nomad-client-3
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 600
- slug: sentinel-policies
  id: s3crog1t2mdz
  type: challenge
  title: Create Nomad Sentinel Policies
  teaser: |
    Create 3 Nomad Sentinel policies to restrict drivers, Docker images, and Docker networks.
  notes:
  - type: text
    contents: |-
      In this challenge, you will create 3 Nomad Sentinel policies:
        * allow-docker-and-java-drivers.sentinel restricts which Nomad drivers can be used.
        * restrict-docker-images.sentinel limits the Docker images that can be used by the Docker driver.
        * prevent-docker-host-network.sentinel prevents Docker containers from using the host network of the Nomad clients they run on.
  assignment: |-
    In this challenge, you will create 3 Nomad Sentinel policies that will restrict which Nomad drivers can be used, which Docker images can be used by the Docker driver, and prevent Docker containers from using the host network of the Nomad clients they run on.

    Start by reviewing the "allow-docker-and-java-drivers.sentinel", "restrict-docker-images.sentinel", and "prevent-docker-host-network.sentinel" Sentinel policies in the sentinel directory on the "Config Files" tab.

    The "allow-docker-and-java-drivers" Sentinel policy only allows Nomad's Docker and Java task drivers to be used in jobs.

    The "restrict-docker-images" Sentinel policy restricts the Docker images that can be used in jobs to the nginx and mongo images which run a specific web server and database respectively. The policy also mandates that a specific tag starting with a number be specified, preventing the "latest" tag from being used. This avoids nasty surprises when a new release is added.

    The "prevent-docker-host-network" Sentinel policy prevents Docker containers from using the host network of the Nomad clients they are run on.

    You will create the first of these Sentinel policies with the "hard-mandatory" enforcement level and the others with the "soft-mandatory" enforcement level. This means that absolutely no Nomad task drivers except for the Docker and Java drivers will be allowed but that Nomad administrators will be able to override failures of the other two Sentinel policies.

    Navigate to the /root/nomad/sentinel directory on the "Server" tab with this command:
    ```
    cd /root/nomad/sentinel
    ```

    Add the "allow-docker-and-java-drivers" Sentinel policy on the "Server" tab with this command:
    ```
    nomad sentinel apply -description "Only allow the Docker and Java drivers" -level hard-mandatory allow-docker-and-java-drivers allow-docker-and-java-drivers.sentinel
    ```
    This should return 'Successfully wrote "allow-docker-and-java-drivers" Sentinel policy!'.

    Add the "restrict-docker-images" Sentinel policy on the "Server" tab with this command:
    ```
    nomad sentinel apply -description "Restrict allowed Docker images" -level soft-mandatory restrict-docker-images restrict-docker-images.sentinel
    ```
    This should return 'Successfully wrote "restrict-docker-images" Sentinel policy!'.

    Add the "prevent-docker-host-network" Sentinel policy on the "Server" tab with this command:
    ```
    nomad sentinel apply -description "Prevent Docker containers running with host network mode" -level soft-mandatory prevent-docker-host-network prevent-docker-host-network.sentinel
    ```
    This should return 'Successfully wrote "prevent-docker-host-network" Sentinel policy!'.

    To become familiar with the Nomad CLI commands for Sentinel policies, we suggest you run the following commands on the "Server" tab:
    ```
    nomad sentinel list
    nomad sentinel read restrict-docker-images
    ```

    The first will list the 3 Sentinel policies you created while the second will show the "restrict-docker-images" policy including its name, scope, enforcement level, description, and its actual policy (rules).

    In the next challenge, you will run jobs in the 3 namespaces and see how the resource quotas and Sentinel policies restrict what they can do.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 600
- slug: run-nomad-jobs-1
  id: tiiro7dy0asi
  type: challenge
  title: Run Nomad Jobs Restricted by ACLs and Sentinel Policies
  teaser: |
    Run Nomad jobs and learn how Nomad ACL and Sentinel policies restrict them.
  notes:
  - type: text
    contents: |-
      In this challenge, you will run Nomad jobs and learn how Nomad ACL and Sentinel policies restrict them.

      You will see that some jobs cannot be run at all because they violate Sentinel policies and that other jobs cannot be run by certain users because their ACL tokens do not allow them to run jobs in the target namespaces.

      In the next challenge, you will see how resource quotas can also prevent certain jobs from running that would not be blocked by Nomad ACL or Sentinel policies.
  assignment: |-
    In this challenge, you will run Nomad jobs and learn how Nomad ACL and Sentinel policies restrict them.

    Navigate to the /root/nomad/jobs directory by running this command on the "Server" tab:
    ```
    cd /root/nomad/jobs
    ```

    ## Run the sleep.nomad Job
    Start by inspecting the "sleep.nomad" job in the jobs directory on the "Config Files" tab. This job uses Nomad's [exec](https://nomadproject.io/docs/drivers/exec) driver to run the Linux `sleep` command.

    If you run `echo $NOMAD_TOKEN`, you should see the bootstrap token that you added to /root/.bash_profile earlier. This means that any jobs you run while it is set will be run with the bootstrap token.

    Try running the sleep.nomad job (with the bootstrap token) with this command on the "Server" tab:
    ```
    nomad job run sleep.nomad
    ```

    Since the exec driver is not allowed by the "allow-docker-and-java-drivers" Sentinel policy, this command should return a red error message including the following text:<br>
    `
    Error submitting job: Unexpected response code: 500 (1 error occurred:
        * allow-docker-and-java-drivers : Result: false
    `<br>

    This indicates that running the job was blocked by the hard-mandatory "allow-docker-and-java-drivers" policy.

    ## Run the catalogue.nomad Job
    Next, inspect the "catalogue.nomad" job on the "Configuration Files" tab. This job launches two Docker containers from custom Docker images, one that runs a web service and one that runs a MySQL database. Neither of these images is allowed by the "restrict-docker-images" policy. Additionally, the containers have their `network_mode` set to `host` which is not allowed by the "prevent-docker-host-network" policy.

    Try running the "catalogue.nomad" job with this command on the "Server" tab:
    ```
    nomad job run catalogue.nomad
    ```

    This should return a red error message including the following text:<br>
    `
    Error submitting job: Unexpected response code: 500 (1 error occurred:
        * prevent-docker-host-network : Result: false
    `<br>

    You might wonder why there is not an error about the Docker images not being allowed. This is because Nomad happened to have evaluated the "prevent-docker-host-network" policy before the "restrict-docker-images" policy and stops evaluating Sentinel policies as soon as one of them fails.

    If you edit the "catalogue.nomad" job to change the value of `network_mode` from `host` to `bridge` and re-run the job, you will see an error message complaining about the "restrict-docker-images" policy being violated. You could make that change with the Instruqt text editor on the "Config Files" tab or run this commands:
    ```
    sed -i 's/"host"/"bridge"/g' /root/nomad/jobs/catalogue.nomad
    ```
    and then run the job:
    ```
    nomad job run catalogue.nomad
    ```

    Since the bootstrap ACL token can override soft-mandatory violations, let's try running the "catalogue.nomad" job again with the `-policy-override` argument after changing the job specification to set `network_mode` back to `host`:
    ```
    sed -i 's/"bridge"/"host"/g' /root/nomad/jobs/catalogue.nomad
    ```
    Then run the job:
    ```
    nomad job run -policy-override catalogue.nomad
    ```
    This returns yellow messages complaining about both violated Sentinel policies, but allows the job to be deployed.

    You can verify its successful deployment in the Nomad UI, after clicking on the "ACL Tokens" menu in the upper-right corner of the Nomad UI, entering your bootstrap token in the "Secret ID" field, clicking the "Set Token" button, and then clicking on the "Jobs" menu on the left-side menu. Note that you might want to make your browser window wider or click on the rectangular icon above the Nomad UI to hide the Instruqt assignment.

    ## Run the webserver-test.nomad Job
    Next, inspect the "webserver-test.nomad" job on the "Config Files" tab. This job uses the Docker task driver to run the Apache web server from the "httpd" image which is not one of our allowed images. Additionally, the `namespace` attribute is set to `qa`, indicating that this job should be run in the "qa" namespace.

    We would like you to run the "webserver-test.nomad" job with Alice's ACL token. Recall that Alice and her token were given the "dev" ACL policy which is allowed to run jobs in the "dev" namespace but not in the "qa" namespace.

    Set Alice's ACL token with this command on the "Server" tab:
    ```
    export NOMAD_TOKEN=$(cat /root/nomad/acls/alice-token.txt | grep Secret | cut -d' ' -f7)
    ```

    Now, run the "webserver-test.nomad" job as Alice with this command:
    ```
    nomad job run webserver-test.nomad
    ```
    This should return a red message, "Error submitting job: Unexpected response code: 403 (Permission denied)".

    This indicates that Alice's ACL token does not permit her to run the job. Note that Nomad does not complain about the violation of the "restrict-docker-images" Sentinel policy because Nomad only evaluates Sentinel policies if the ACL policies associated with the user running the job allow them to run it. In this case, Alice's ACL token does not have an ACL policy allowing her to run jobs in the "qa" namespace, so no Sentinel policies are evaluated.

    Recall that Bob's ACL token was assigned the "qa" ACL policy. So, let's try setting his ACL token and running the job again. Do that with these commands:
    ```
    export NOMAD_TOKEN=$(cat /root/nomad/acls/bob-token.txt | grep Secret | cut -d' ' -f7)
    ```
    and
    ```
    nomad job run webserver-test.nomad
    ```

    This time, you should see a red message complaining about the "restrict-docker-images" Sentinel policy being violated. Since we did not see a 403 error, this means that Bob would be allowed to run the job if it did not violate any Sentinel policies.

    Bob might be tempted to run the job with the `-policy-override` argument. Go ahead and have Bob try this by running this command:
    ```
    nomad job run -policy-override webserver-test.nomad
    ```
    You will see the red error message "Error submitting job: Unexpected response code: 403 (Permission denied)" because Bob's ACL policy does not allow him to override violations of soft-mandatory Sentinel policies. Later, you'll see that Charlie can do this.

    Having learned that he can't circumvent Nomad's Sentinel policies, Bob decides to run the nginx web server instead of the Apache web server. He can do this by editing the "webserver-test.nomad" job to specify the "nginx" image instead of the "httpd" image. Have him do this either by editing the job with the Instruqt text editor on the "Config Files" tab or by running this command:
    ```
    sed -i "s/httpd/nginx/g" webserver-test.nomad
    ```

    After editing the job, run it again with this command:
    ```
    nomad job run webserver-test.nomad
    ```
    You might or might not be surprised to see a red message complaining about the "restrict-docker-images" Sentinel policy being violated. What's the problem?

    Recall that the "restrict-docker-images" Sentinel policy not only resticts the allowed Docker images but requires them to have a tag starting with a number. But Bob set the image to "nginx" without a tag.

    You can fix this by adding the tag "1.15.6" with this command:
    ```
    sed -i "s/nginx/nginx:1.15.6/g" webserver-test.nomad
    ```

    Now, Bob can finally run the "webserver-test.nomad" job without errors with this command:
    ```
    nomad job run webserver-test.nomad
    ```

    If you look at the Nomad UI, you will not see the new job at first. This is because you are currently looking at jobs in the "default" namespace while Bob ran the job in the "qa" namespace.

    You can fix this by clicking the refresh icon above the Nomad UI and selecting the "qa" namespace in the workload drop-down menu. You should now see the webserver-test job running in the Nomad UI. If you switch back to the "Default Namespace", you will again see the "catalogue" job running.

    ## Overriding Sentinel Violations
    After running his website on nginx for a few days, Bob realizes that some pages are not loading correctly. He plans on making some code changes to fix them, but would like to revert to using the Apache web server (httpd) until then. So, he asks the infrastructure manager, Charlie, for an exemption to the Sentinel policy that requires nginx. Fortunately, Charlie agrees.

    Have Charlie revert the "webserver-test.nomad" job specification to use "httpd" with this command:
    ```
    sed -i "s/nginx:1.15.6/httpd:2.4/g" webserver-test.nomad
    ```

    Then set the `NOMAD_TOKEN` environment variable to Charlie's token:
    ```
    export NOMAD_TOKEN=$(cat /root/nomad/acls/charlie-token.txt | grep Secret | cut -d' ' -f7)
    ```

    Finally, have Charlie run the webserver-test job with the `-policy-override` argument:
    ```
    nomad job run -policy-override webserver-test.nomad
    ```
    This should give a yellow warning about the violation of the "restrict-docker-images" Sentinel policy but successfully redeploy the job, switching the web server from nginx to httpd.

    Wait until both of the new allocations of the webserver-test job are running before clicking the "Check" button.

    In this challenge, you saw how Nomad ACL and Sentinel policies restricted which jobs could be run and who could run them.

    In the next challenge, you will run some more jobs and see how they are affected by resource quotas.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 900
- slug: run-nomad-jobs-2
  id: d3xyecyjqxwc
  type: challenge
  title: Run Nomad Jobs Restricted by Resource Quotas
  teaser: |
    Learn how Nomad Resource Quotas can prevent some jobs from running all of their desired allocations.
  notes:
  - type: text
    contents: |-
      In this challenge, you will run some more Nomad jobs and learn how Resource Quotas can prevent some of their allocations from running.

      These are jobs that Nomad ACLs and Sentinel policies would allow to run if the namespaces they target were not already overloaded.

      You will also see that stopping a job in a namespace can allow a queued allocation to be deployed.
  assignment: |-
    In this challenge, you will run some more Nomad jobs and learn how Resource Quotas can prevent some of them from running all of their desired allocations.

    Navigate to the /root/nomad/jobs directory by running this command on the "Server" tab:
    ```
    cd /root/nomad/jobs
    ```

    Next, export Alice's Nomad ACL token with this command:
    ```
    export NOMAD_TOKEN=$(cat /root/nomad/acls/alice-token.txt | grep Secret | cut -d' ' -f7)
    ```

    Have Alice inspect the current usage of the "dev" resource quota with this command:
    ```
    nomad quota status dev
    ```
    This will return the following:<br>
    `
    Name        = dev
    Description = dev quota
    Limits      = 1
    Quota Limits
    Region  CPU Usage  Memory Usage  Network Usage
    global  0 / 2300   0 / 4100      - / inf
    `<br>
    which indicates that none of the "dev" resource quota is being used. ("inf" under "Network Usage" stands for infinity and means that there is no limit on network usage in the "dev" resource quota.)

    Inspect the "website-dev.nomad" job specification file in the jobs directory on the "Config Files" tab and note that it wants to run a job called "website" in the "dev" namespace. The job consists of "nginx" and "mongodb" task groups that each have one task that would consume 250MHz of CPU and 512MB of memory.

    However, the count for each task group is 2, so the job would consume a total of 1,000MHz of CPU and 2,048MB of RAM if Nomad is able to schedule all four tasks. Since there are currently no jobs running in the "dev" namespace and these amounts are lower than the limits of the "dev" resource quota, Nomad should be able to run the entire job.

    See if that is true by running the job as Alice with this command:
    ```
    nomad job run website-dev.nomad
    ```

    After waiting 30 seconds, check the status of the job with this command:
    ```
    nomad job status -namespace=dev website
    ```
    Note that you need to specify the namespace since job names are not unique across multiple namespaces.

    You should see 4 allocations running at the bottom. You can also check the status of the "website" job in the "dev" namespace in the Nomad UI after providing a suitable ACL token after selecting the "ACL Tokens" menu; we recommend using Charlie's token from the charlie-token.txt file in the /root/nomad/acls directory.

    You'll then need to select the "Jobs" menu and click the Instruqt refresh icon to make the Nomad UI show the namespace selector which will let you select the "dev" namespace. If you click on the "website" job in the "dev" namespace, you will see 4 Desired, Placed, and Healthy allocations.

    Check the status of the dev resource quota on the "Server" tab again to see how much of the "dev" resource quota is being used:
    ```
    nomad quota status dev
    ```
    You should now see this:<br>
    `
    Name        = dev
    Description = dev quota
    Limits      = 1
    Quota Limits
    Region  CPU Usage    Memory Usage  Network Usage
    global  1000 / 2300  2048 / 4100   40 / inf
    `<br>
    which exactly matches what we had expected.

    Next, inspect the "website-qa.nomad" job specfication on the "Config Files" tab. It is very similar to the "website-dev.nomad" job specification and even uses the same job name, "website", but it targets the "qa" namespace and wants to use 1,024MB of memory for each task. This means it will want to use a total of 4,096MB of memory.

    Export Bob's Nomad ACL token with this command:
    ```
    export NOMAD_TOKEN=$(cat /root/nomad/acls/bob-token.txt | grep Secret | cut -d' ' -f7)
    ```

    Then, have Bob check the current usage in the "qa" namespace with this command:
    ```
    nomad quota status qa
    ```
    This will return the following:<br>
    `
    Name        = qa
    Description = qa quota
    Limits      = 1
    Quota Limits
    Region  CPU Usage    Memory Usage  Network Usage
    global  500 / 2300  1024 / 4100   20 / inf
    `<br>

    Since 1,024MB of memory is already being used and the "qa" resource quota limits the "qa" namespace to a total of 4,100MB and the "website-qa.nomad" needs 4,096MB, Nomad clearly cannot schedule all 4 allocations of the job. (There is enough CPU capacity.)

    Even so, have Bob try to run it with this command:
    ```
    nomad job run website-qa.nomad
    ```
    This will return something like this:
    ```
    ==> Monitoring evaluation "17faede9"
        Evaluation triggered by job "website"
        Evaluation within deployment: "ecfe5ffc"
        Allocation "718aae72" created: node "89683da5", group "nginx"
        Allocation "5452e158" created: node "a9e882e3", group "nginx"
        Allocation "5f7186bd" created: node "a9e882e3", group "mongodb"
        Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "17faede9" finished with status "complete" but failed to place all allocations:
        Task Group "mongodb" (failed to place 2 allocations):
          * Resources exhausted on 1 nodes
          * Dimension "memory" exhausted on 1 nodes
          * Quota limit hit "memory exhausted (5120 needed > 4100 limit)"
        Evaluation "f94e7499" waiting for additional capacity to place remainder
    ```

    Note that the Nomad failed to place 2 allocations because the running job and the four allocations of the new job would need a total of 5,120MB which is greater than the 4,096MB limit of the "qa" resource quota. (Sometimes, the message might say Nomad failed to place 1 allocation; and if you inspect the job in the Nomad UI, you might actually see 3 allocations running.)

    You can also select the "qa" namespace in the Nomad UI, select the "website" in it, and confirm that only 3 of the 4 allocations of the "website" job are running while 1 of them is queued. The fact that the allocation is currently queued rather than failed is important; this means that it could still be deployed if extra memory became available within the "qa" namespace or if the memory limit of the "qa" resource quota were increased.

    To see this actually happen, stop the "webserver-test" job by running this command on the "Server" tab:
    ```
    nomad job stop -namespace=qa webserver-test
    ```
    and quickly switch back to the Nomad UI. Over the next 15-30 seconds, you should see the "Placed" number and then the "Healthy" number both change to 4, showing that Nomad completed the deployment of the "website" job in the "qa" namespace after extra memory was made available by stopping the "webserver-test" job in the same namespace.

    Recall from the first challenge that the total capacity of the 3 Nomad clients in your cluster is 11.25GB of memory and 6,900MHz of CPU capacity. So, your cluster ostensibly has enough memory on the 3 Nomad clients to run both the "website" and the "webserver-test" jobs in the "qa" namespace along with the "website" job in the "dev" namespace and the "catalogue" job in the "default" namespace.

    However, it is important to keep in mind that some memory and CPU capacity is needed by the OS and the Nomad client itself on each of the 3 Nomad clients. So, if Nomad had allowed the QA team to run both the "website" and "webserver-test" jobs in the "qa" namespace, the Dev team might have been adversely affected if they had later tried to run some other jobs.

    Finally, to see the status of the website jobs in both the Dev and the QA namespaces, run these command:
    ```
    export NOMAD_TOKEN=$(cat /root/nomad/acls/bootstrap.txt | grep Secret | cut -d' ' -f7)
    nomad job status -namespace=* website
    ```
    This uses Nomad Enterprise's [Cross-Namespace Queries](https://www.nomadproject.io/docs/enterprise#cross-namespace-queries) feature.

    It should return data like this:
    ```
    Prefix matched multiple jobs

    ID       Namespace  Type     Priority  Status   Submit Date
    website  dev        service  40        running  2020-07-24T03:10:34Z
    website  qa         service  50        running  2020-07-24T03:11:55Z
    ```
    Cross-namespace queries like this allow Nomad operators to quickly do queries across multiple namespaces and then run other namespace-specific queries to get more details.

    Note that using `*` in a cross-namespace query will only return data from namespaces that the current ACL token has access to. That is why we first exported the bootstrap token.

    In this challenge, you saw how resource quotas can prevent jobs that violate them from running all of their desired allocations. You also saw that stopping a job in a namespace can allow a queued allocation to be deployed.

    Congratulatons on completing the Nomad Enterprise Governance track!
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 900
checksum: "10478344320025986821"
