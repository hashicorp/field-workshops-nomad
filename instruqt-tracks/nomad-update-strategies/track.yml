slug: nomad-update-strategies
id: orxif8plq08a
type: track
title: Nomad Job Update Strategies
teaser: |
  Explore Nomad job update strategies including rolling, blue/green, and canary updates.
description: |-
  Most applications are long-lived and require updates over time. Whether you are deploying a new version of your web application or updating to a new version of a database, Nomad has built-in support for rolling, blue/green, and canary updates, which can be used with non-containerized and containerized applications or to containerize the former.

  This track will guide you through implementing these update strategies based on the [Nomad Job Update Strategies](https://learn.hashicorp.com/tutorials/nomad/job-update-strategies) guide.

  You will run "mongodb", "chat-app", and "nginx" jobs and update the second using rolling, blue/green, and canary update strategies.

  NGINX will be used as a load balancer that will forward requests to instances of the "chat" application which will store its chat messages in a single MongoDB database.

  This track will demonstrate Nomad's job update strategies using a single web app that will initially be deployed by Nomad's [exec](https://www.nomadproject.io/docs/drivers/exec/) task driver as a binary compiled from node.js. The first version of the app will have a light background. You will then use a rolling update to convert to a version of the binary that uses a dark background. You will then do a blue/green deployment to convert to a Docker version of the app that uses the light background. Finally, you will do a canary deployment that converts the Docker version of the app to the dark background.

  You will also learn how Nomad allows you to promote or fail blue/green and canary deployments.

  Before running this track, we suggest you run the **Nomad Basics**, **Nomad Simple Cluster**, and **Nomad Multi-Server Cluster** tracks.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- Nomad
- updates
- rolling
- blue/green
- canary
- dockerized apps
- legacy apps
owner: hashicorp
developers:
- roger@hashicorp.com
private: true
published: true
show_timer: true
challenges:
- slug: verify-nomad-cluster-health
  id: tklzqhaey58k
  type: challenge
  title: Verify the Health of Your Nomad Cluster
  teaser: |
    Verify the health of the Nomad cluster that has been deployed for you.
  assignment: |-
    In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients running Nomad 1.0.0 and Consul 1.9.0.

    The [Consul Connect integration](https://www.nomadproject.io/docs/integrations/consul-connect/) has been enabled on the server and clients since the chatapp uses it to talk to the MongoDB database. Additionally, a Nomad [host volume](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) called "mongodb_mount" has been configured on the clients under the /opt/mongodb/data directory so that MongoDB can persist its data.

    First, verify that all 4 Consul agents are running and connected to the cluster by running this command on the "Server" tab:
    ```
    consul members
    ```
    You should see 4 Consul agents with the "alive" status.

    Check that the Nomad server is running by running this command on the "Server" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server with the "alive" status.

    Check the status of the Nomad client nodes by running this command on the "Server" tab:
    ```
    nomad node status
    ```
    You should see 3 Nomad clients with the "ready" status.

    You can also check the status of the Nomad server and clients in the Nomad and Consul UIs.

    In the next challenge, you will will run "mongodb", "chat-app", and "nginx" jobs.
  notes:
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

      In later challenges, you will run "mongodb", "chat-app", and "nginx" jobs and update the second using rolling, blue/green, and canary update strategies. NGINX will be used as a load balancer that will forward requests to instances of the "chat" application which will store its messages in a single MongoDB database.
  - type: text
    contents: |-
      You will start with a non-containerized version of the "chat-app" application that uses Nomad's [exec](https://www.nomadproject.io/docs/drivers/exec/) task driver to run a binary compiled from a node.js application that uses a light background.

      You will then use a rolling update to convert to a version of the binary that uses a dark background.

      You will then do a blue/green deployment to convert to a Docker version of the app that uses the light background.

      Finally, you will do a canary deployment that converts the Docker version of the app to the dark background.

      You will also learn how Nomad allows you to promote or fail blue/green and canary deployments.
  tabs:
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 600
- slug: deploy-the-jobs
  id: wfbxeo0th19f
  type: challenge
  title: Deploy the MongoDB, NGINX, and Chat-App Jobs
  teaser: |
    Deploy the MongoDB, NGINX, and Chat-App jobs.
  assignment: |-
    In this challenge, you will deploy the "mongodb", "chat-app", and "nginx" jobs.

    ## Run the mongodb.nomad Job
    Inspect the "mongodb.nomad" job specification file on the "Jobs" tab. This will deploy a "db" task group that runs the MongoDB database on port 27017 from the "mongo" Docker image.

    Note that the database persists its data to the Nomad volume "mongodb_vol" which uses the "mongodb_mount" host volume that was configured on each Nomad client. The "mongodb_vol" volume is mounted inside the Docker container on the path "/data/db".

    Navigate to the /root/nomad/jobs directory on the "Server" tab with this command:
    ```
    cd /root/nomad/jobs
    ```

    Run the "mongodb.nomad" job with this command on the "Server" tab:
    ```
    nomad job run mongodb.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "64754ee4"
    Evaluation triggered by job "mongodb"
    Evaluation within deployment: "baa2ea28"
    Allocation "60fde323" created: node "6777b2ca", group "db"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "64754ee4" finished with status "complete"
    `<br>

    You can check the job in the Nomad UI after waiting 15-30 seconds, clicking the Instruqt refresh button above the Nomad UI, and then selecting the "mongodb" job. You might need to make your browser window a bit wider to see the UI properly and so that the Instruqt buttons do not overlap the Nomad UI tab. You could also click the rectangular button next to the refresh button to hide this assignment. After no more than 1 minute, you should see that the job has 1 healthy allocation.

    Alternatively, you can run the command:
    ```
    nomad job status mongodb
    ```
    on the "Server" tab to check the status of the "mongodb" job and validate that the "db" task group is healthy.

    ## Run the chat-app Job
    Next, inspect the "chat-app-light-binary.nomad" job specification file on the "Jobs" tab. This job deploys 3 instances of a "chat-app" task group that runs the "chatapp-light-linux" binary compiled from a node.js application that was created by a HashiCorp solutions engineer, Guy Barrows.

    It runs the app using Nomad's [exec](https://www.nomadproject.io/docs/drivers/exec/) task driver. In the next challenge, we will update the app to a different binary that runs a dark version of the app. Here is the relevant portion of the job specification:
    ```
    task "chat-app" {
      driver = "exec"

      config {
        command = "chatapp-light-linux"
      }

      artifact {
        source = "https://github.com/GuyBarros/anonymouse-realtime-chat-app/releases/download/0.03/chatapp-light-linux"
        options {
          checksum = "md5:55677699984200530a836cf8fdec5bb5"
        }
      }
    }
    ```

    You can see that we are downloading the binary from a Google Cloud Platform storage bucket using Nomad's [artifact](https://www.nomadproject.io/docs/job-specification/artifact/) stanza.

    One instance of the chat-app task group will be deployed to each Nomad client because of the job's use of the "spread" stanza:<br>
    `
    spread {
      attribute = "${node.unique.name}"
    }
    `<br>
    Later, when we do a blue-green deployment, we will see that the new "green" deployments will also be spread evenly across the 3 Nomad clients.

    The "chat-app" task runs on a dynamic port selected by Nomad; that port is mapped to port 5000 inside the task group's "network" stanza:

    ```
    network {
      mode = "bridge"
      port "http" {
        to = 5000
      }
    }
    ```

    The NGINX load balancer that you will deploy after the "chat-app" job will automatically add a route for each instance of the chat app because of the "template" stanza within the "nginx.nomad" job specification file.

    Finally, the "chat-app" task group uses Consul Connect [sidecar proxies](https://www.consul.io/docs/connect/proxies.html) to talk to the MongoDB database using mutual Transport Layer Security (mTLS) certificates. This is implemented by this stanza:

    ```
    connect {
      sidecar_service {
        tags = ["chat-app-proxy"]
        proxy {
          upstreams {
            destination_name = "mongodb"
            local_bind_port = 27017
          }
        }
      }
    }
    ````

    The exact same stanza will be used in versions of the job that run the containerized versions of the application with the [Docker](https://www.nomadproject.io/docs/drivers/docker/) task driver. Nomad and Consul Connect work equally well with non-containerized and containerized applications.

    Run the "chat-app-light-binary.nomad" job on the "Server" tab with this command:
    ```
    nomad job run chat-app-light-binary.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "f08ca561"
    Evaluation triggered by job "chat-app"
    Evaluation within deployment: "5dc975d3"
    Allocation "5d57baab" created: node "9afadcdd", group "chat-app"
    Allocation "14a5cc97" created: node "6777b2ca", group "chat-app"
    Allocation "2f792469" created: node "4b19ed9a", group "chat-app"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "f08ca561" finished with status "complete"
    `<br>

    After waiting about 60 seconds, check the status of the "chat-app" job with this command on the "Server" tab:
    ```
    nomad job status chat-app
    ```

    This should indicate that the job is running and have an Allocations section at the bottom that looks like this:<br>
    `
    Allocations
    ID        Node ID   Task Group  Version  Desired  Status   Created    Modified
    14a5cc97  6777b2ca  chat-app    0        run      running  1m11s ago  24s ago
    2f792469  4b19ed9a  chat-app    0        run      running  1m11s ago  16s ago
    5d57baab  9afadcdd  chat-app    0        run      running  1m11s ago  16s ago
    `<br>

    Note that the Node IDs of the 3 allocations are all different. In other words, Nomad scheduled one instance of the chat-app task to each Nomad client because of the `spread` stanza we included in the "chat-app-light-binary.nomad" job specification.

    You can also check out the "chat-app" job in the Nomad UI. Within 60 seconds of running the job, all 3 allocations should be healthy.

    You can also verify that the Nomad task groups were automatically registered as Consul services by looking at the "Services" tab of the Consul UI. Note the sidecar proxy services created by Nomad's integration with Consul Connect. The services all include node checks and service checks.

    ## Run the nginx.nomad Job
    Next, inspect the "nginx.nomad" job specification file on the "Jobs" tab. This job deploys [NGINX](https://nginx.org) as a system job on all the Nomad clients from a Docker image. Since it is a system job, no "count" is specified for the "nginx" task group.

    NGINX will route requests to the "chat-app" Consul service registered by the "chat-app-light-binary.nomad" job; this is done by the "template" stanza of the job which generates the NGINX configuration file, "load-balancer.conf", based on the current instances of the "chat-app" service. (This explains why we run the "nginx.nomad" job after the "chat-app-light-binary.nomad" job.)

    The "ip_hash" instruction in the "load-balancer.conf" configuration file is included to make the NGINX sessions of the chat app sticky. This is required to avoid HTTP 400 errors that would otherwise occur when multiple requests are sent during the lifetime of a socket used by the socket.io framework. (See this [doc](https://socket.io/docs/using-multiple-nodes) for details.)

    Run the "nginx.nomad" job on the "Server" tab with this command:
    ```
    nomad job run nginx.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "32fbe07d"
    Evaluation triggered by job "nginx"
    Allocation "38f8f053" created: node "9afadcdd", group "nginx"
    Allocation "58e6b570" created: node "6777b2ca", group "nginx"
    Allocation "683d7322" created: node "4b19ed9a", group "nginx"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "32fbe07d" finished with status "complete"
    `<br>

    You can now access the chat-app UI on the "Chat 1", "Chat 2", and "Chat 3" tabs. Note that all instances of the chat app currently have a light background.

    Since we are using NGINX, the 3 Chat tabs are not actually pinned to specific Nomad clients. In fact, all of them are pointing at the NGINX load balancer itself on the corresponding Nomad client. Each time you click the refresh button for any of the Chat tabs, you will be randomly connected to an instance of the chat app on any of the 3 Nomad clients in a new session. As mentioned above, however, each session of the chat app between refreshes is sticky.

    If you type messages in the Chat tabs, they will usually show up in the other tab. However, you might sometimes need to click the Instruqt refresh button to the right of all the tabs while one of the Chat tabs is selected in order to force it to reconnect to the database. This is an issue with the chat app itself rather than with Nomad.

    In the next challenge, you will update the Chat app to use a dark background using Nomad's rolling update strategy.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy the "mongodb", "chat-app", and "nginx" jobs.

      The first version of the "chat-app" job uses Nomad's [exec](https://www.nomadproject.io/docs/drivers/exec/) task driver to run a binary compiled from a node.js application. This version of the app has a light background.

      In the next challenge, you will update the Chat app to a different binary that uses a dark background using the rolling update strategy.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 8080
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 8080
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 8080
  difficulty: basic
  timelimit: 900
- slug: rolling-update
  id: s71yve1la3vm
  type: challenge
  title: Do a Rolling Update of the Chat Job
  teaser: |
    Do a rolling update of the Chat job in which you replace one instance at a time.
  assignment: |-
    In this challenge, you will do a rolling update of the "chat-app" job.  You will be replacing the chatapp-light-linux binary with a chatapp-dark-linux binary that uses a dark background. This binary is also run with Nomad's exec task driver.

    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Please inspect the "chat-app-dark-binary.nomad" job specification file on the "Jobs" tab and note that the setup script for this challenge has changed the task as follows:
    ```
    task "chat-app" {
      driver = "exec"

      config {
        command = "chatapp-dark-linux"
      }

      artifact {
        source = "https://github.com/GuyBarros/anonymouse-realtime-chat-app/releases/download/0.03/chatapp-dark-linux"
        options {
          checksum = "md5:912f8476a5f4fff479121b11bff5e098"
        }
      }
    }
    ```
    The command that will be run by the exec driver and the source of the artifact have been changed. Note that the name of the job at the top of the file is still "chat-app"; so running this job will actually update the job with that name that we initially created with the "chat-app-light-binary.nomad" job specification file.

    Additionally, the job's `update` stanza looks like this:
    ```
    update {
      max_parallel = 1
      health_check = "checks"
      min_healthy_time = "15s"
      healthy_deadline = "2m"
    }
    ```

    This tells Nomad that it should update one instance of the task group at a time, base the health of each instance on its health checks, require a new allocation to be healthy for 15 seconds before marking it as healthy, and require a new allocation to be healthy after no more than 2 minutes; if an allocation is not healthy within 2 minutes, it will be marked as unhealthy.

    Now, check what would happen if you redeployed the "chat-app" job with this command:
    ```
    nomad job plan chat-app-dark-binary.nomad
    ```

    This should return the following text:<br>
    `
    +/- Job: "chat-app"
    +/- Task Group: "chat-app" (1 create/destroy update, 2 ignore)
      +/- Task: "chat-app" (forces create/destroy update)
        +/- Config {
          +/- command: "chatapp-light-linux" => "chatapp-dark-linux"
            }
        +   Artifact {
          + GetterMode:              "any"
          + GetterOptions[checksum]: "md5:912f8476a5f4fff479121b11bff5e098"
          + GetterSource:            "https://github.com/GuyBarros/anonymouse-realtime-chat-app/releases/download/0.03/chatapp-dark-linux"
          + RelativeDest:            "local/"
          }
        -   Artifact {
          - GetterMode:              "any"
          - GetterOptions[checksum]: "md5:55677699984200530a836cf8fdec5bb5"
          - GetterSource:            "https://github.com/GuyBarros/anonymouse-realtime-chat-app/releases/download/0.03/chatapp-light-linux"
          - RelativeDest:            "local/"
          }
        Task: "connect-proxy-chat-app"
    `<br>
    This indicates that Nomad would initially only update 1 of the 3 allocations, confirming that it would do a rolling update. Note that the URLs and checksums of the binaries are different.

    Go ahead and actually run a rolling update of the chat application:
    ```
    nomad job run chat-app-dark-binary.nomad
    ```

    In the Nomad UI, if you select the "chat-app" job, you will see that there is a new deployment running. Over the next two minutes, you will see the numbers of placed and healthy allocations for this deployment increase to 3.

    As the number of healthy allocations increases, refresh the "Chat 1", "Chat 2", and "Chat 3" tabs periodically. Initially, you will only see the light background; but over the next 2 minutes as the rolling update proceeds, you will see the dark background 1/3 of the time, then 2/3 of the time, and then all the time.

    Since each instance of the chat app is getting its messages from MongoDB, the old messages will show up in the instances of the chat app with the dark background.

    After all the chat-app allocations are healthy and the chat-app clients only show the dark background, run the following command on the "Server" tab:
    ```
    nomad job status chat-app
    ```
    You will now see 6 allocations, 3 of which are marked "complete" while 3 are "running".

    In the next challenge, you will update the Chat app to a Docker-based version that uses the original light background using a "blue/green" deployment strategy.
  notes:
  - type: text
    contents: |-
      In this challenge, you will do a rolling update of the Chat job to make it use a dark background instead of the current light background.

      The new version of the app is a different node.js binary that is also run by Nomad's exec driver.

      In the next challenge, you will update the Chat app to a Docker-based version that uses the original light background using a "blue/green" deployment strategy.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 8080
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 8080
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 8080
  difficulty: basic
  timelimit: 900
- slug: blue-green-deployment
  id: vyyv312rchyo
  type: challenge
  title: Do a Blue/Green Deployment of the Chat Job
  teaser: |
    Do a blue/green deployment of the Chat job.
  assignment: |-
    In this challenge, you will do a blue/green deployment of the Chat job in which you deploy 3 new instances alongside the current ones and then promote the deployment when satisfied.

    This means that you will initally deploy 3 new Docker-based instances of the chat app with the original light background but keep the 3 non-Docker instances with the dark background running. When you are satisfied with the new instances, you will promote the deployment, causing Nomad to stop the old instances.

    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Inspect the new "chat-app-light-docker.nomad" job specification file on the "Jobs" tab and note that the update stanza now has `canary = 3`:

    Additionally, note that the task now uses the Docker driver that runs a custom Docker image, "lhaig/anon-app:light-0.03" which another HashiCorp solutions engineer, Lance Haig, uploaded to Docker Hub for us:
    ```
    task "chat-app" {
      driver = "docker"

      config {
        image = "lhaig/anon-app:light-0.03"
      }
    }
    ```

    Setting the "canary" attribute to the "count" attribute of the "chat-app" task group tells Nomad to deploy 3 new instances of the task group as part of a "blue/green" deployment in which the 3 "blue" instances of the chat app with the dark background continue to run even after the new "green" instances of the chat app with the light background are deployed. (Note that the colors "blue" and "green" here have nothing to do with the background colors of the chat app; they are just standard terminology for referring to this particular update strategy.) In this scenario, we are migrating the Chat App from the non-Docker version to a Docker version.

    Now, check what would happen if you redeployed the "chat-app" job with this command on the "Server" tab:
    ```
    nomad job plan chat-app-light-docker.nomad
    ```

    This should return something like this:<br>
    `
    +/- Job: "chat-app"
    +/- Task Group: "chat-app" (3 canary, 3 ignore)
      +/- Update {
          AutoPromote:      "false"
          AutoRevert:       "false"
      +/- Canary:           "0" => "3"
          HealthCheck:      "checks"
          HealthyDeadline:  "120000000000"
          MaxParallel:      "1"
          MinHealthyTime:   "15000000000"
          ProgressDeadline: "600000000000"
        }
      +/- Task: "chat-app" (forces create/destroy update)
        +/- Driver: "exec" => "docker"
        +/- Config {
          - command: "chatapp-dark-linux"
          + image:   "lhaig/anon-app:light-0.03"
            }
            -   Artifact {
              - GetterMode:              "any"
              - GetterOptions[checksum]: "md5:912f8476a5f4fff479121b11bff5e098"
              - GetterSource:            "https://github.com/GuyBarros/anonymouse-realtime-chat-app/releases/download/0.03/chatapp-dark-linux"
              - RelativeDest:            "local/"
              }
            Task: "connect-proxy-chat-app"
    `<br>

    The plan indicates that Nomad would create 3 canaries to run the "light-0.03" version of a Docker version of the chat app with image "lhaig/anon-app:light-0.03" but ignore the 3 allocations that are currently running. That would result in 6 instances of the chat app running on the 3 Nomad clients. That is ok, however, since we are using ports dynamically selected by Nomad and are using NGINX to forward requests to the chat app instances.

    Go ahead and re-run the "chat-app" job with this command:
    ```
    nomad job run chat-app-light-docker.nomad
    ```

    This should return something like this:<br>
    `
    ==> Monitoring evaluation "81ca49f4"
    Evaluation triggered by job "chat-app"
    Evaluation within deployment: "916c6388"
    Allocation "0803dbf0" created: node "c645cab8", group "chat-app"
    Allocation "b8f8e7c2" created: node "ded26beb", group "chat-app"
    Allocation "e57ae40c" created: node "f9ce3ddf", group "chat-app"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "81ca49f4" finished with status "complete"
    `<br>

    If you look at the chat-app job in the Nomad UI, you will see that there are 6 allocations running. You will also see an orange "Promote Canary" button and a message saying "Deployment is running but requires manual promotion".

    Check the status of the "chat-app" job by running this command:
    ```
    nomad job status chat-app
    ```
    The "Deployed" section will show "false" in the "Promoted" column and will eventually show 3 desired, 3 canaries, 3 placed, and 3 healthy instances. The "Allocations" section at the bottom should show 6 running and 3 stopped allocations. The latter are from when you did the rolling update in the last challenge.

    If you refresh the "Chat 1", "Chat 2", and "Chat 3" tabs, you will see the app with both the light and the dark backgrounds. Each background should show up 50% of the time since all 6 allocations are active at this point.

    Next, you can promote the "green" allocations by running this command:<br>
    `
    nomad deployment promote <deployment_id>
    `<br>
    replacing <deployment_id\> with the deployment ID of the active deployment returned by the job status command. Alternatively, you could click the orange "Promote Canary" button in the Nomad UI.

    After clicking the Instruqt refresh button for the Nomad UI and selecting the "chat-app" job, you should see that the number of running clients has changed to 3. If you run the job status command again, you will now see 3 running and 6 stopped allocations.

    Now, when you refresh the the "Chat 1", "Chat 2", and "Chat 3" tabs, you will only see the version of the chat app with the light background since the allocations with the dark background have been stopped by Nomad.

    If you had been unhappy with the new version of the chat app, you could have rolled back to the old version by running `nomad deployment fail <deployment_id>`, replacing <deployment_id\> with the deployment ID returned by the job status command. Nomad would then have stopped the new allocations leaving the app in the state it had been in before you re-ran the job.

    In the next challenge, you will update the Chat job using a "canary" deployment, but you will rollback the deployment instead of promoting it. You will be changing to a Docker-based version of the app that uses the dark background.
  notes:
  - type: text
    contents: |-
      In this challenge, you will do a "blue/green" deployment of the Chat job for which Nomad will deploy 3 new Docker-based instances of the chat app with the original light background but keep the 3 binary-based instances with the dark background running.

      When you are satisfied with the new version of the app, you will promote the deployment, causing Nomad to stop the old instances.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 8080
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 8080
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 8080
  difficulty: basic
  timelimit: 600
- slug: canary-deployment
  id: xi4uaufnbg0y
  type: challenge
  title: Do a Canary Deployment of the Chat Job
  teaser: |
    Do a canary deployment of the Chat job in which you deploy and evaluate 1 new instance but decide to roll back the deployment.
  assignment: |-
    In this challenge, you will do a "canary" deployment of the chat-app job, but you will rollback the deployment instead of promoting it.

    This means that you will deploy 1 new "canary" instance of a Docker-based version of the chat app with the dark background but keep the 3 instances with the light background that you deployed in the last challenge running. You will then roll back the deployment with the `nomad deployment fail` command.

    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Inspect the chat-app-dark-docker.nomad job specification file on the "Jobs" tab and note that we have changed "canary" from "3" to "1" and changed the image of the task to "lhaig/anon-app:dark-0.03".

    Setting the "canary" attribute to "1" tells Nomad to deploy a single "canary" instance of the chat app with the dark background while leaving the 3 instances with the light background running.

    Now, check what would happen if you redeployed the "chat-app" job with this command on the "Server" tab:
    ```
    nomad job plan chat-app-dark-docker.nomad
    ```

    This should return something like this:<br>
    `
    +/- Job: "chat-app"
    +/- Task Group: "chat-app" (1 canary, 3 ignore)
      +/- Update {
          AutoPromote:      "false"
          AutoRevert:       "false"
      +/- Canary:           "3" => "1"
          HealthCheck:      "checks"
          HealthyDeadline:  "120000000000"
          MaxParallel:      "1"
          MinHealthyTime:   "15000000000"
          ProgressDeadline: "600000000000"
        }
    +/- Task: "chat-app" (forces create/destroy update)
      +/- Config {
        +/- image: "lhaig/anon-app:light-0.03" => "lhaig/anon-app:dark-0.03"
          }
        Task: "connect-proxy-chat-app"
    `<br>

    The plan indicates that Nomad would create 1 canary to run the Docker "lhaig/anon-app:dark-0.03" version of the chat app but ignore the 3 allocations that are currently running. That would result in 4 instances of the chat app running on the 3 Nomad clients.

    Go ahead and re-run the "chat-app" job with this command:
    ```
    nomad job run chat-app-dark-docker.nomad
    ```

    This should return something like this:<br>
    `
    ==> Monitoring evaluation "0fd02bb5"
        Evaluation triggered by job "chat-app"
        Evaluation within deployment: "fa80df0c"
        Allocation "7633b926" created: node "14e04978", group "chat-app"
        Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "0fd02bb5" finished with status "complete"
    `<br>

    If you look at the chat-app job in the Nomad UI, you will see that there are 4 allocations running. You will also see an orange "Promote Canary" button as in the last challenge. Additionally, the active deployment will eventually show "1/1" Canaries, 1 Placed, 3 Desired, and 1 Healthy allocation.

    The fact that the active deployment only has 1 healthy allocation might worry you, but this is actually ok; it just means that Nomad still plans on deploying 2 more allocations with the new version of the app if you do promote the canary. In other words, since the "count" of the "chat-app" task group is 3, Nomad will always want to deploy 3 allocations as part of any deployment of the job. By specifying 1 canary, you forced Nomad to delay the other 2 allocations.

    Check the status of the "chat-app" job by running this command:
    ```
    nomad job status chat-app
    ```
    The "Deployed" section should show "false" in the "Promoted" column along with 3 desired, 1 canary, 1 placed, and (eventually) 1 healthy instances, agreeing with what the active deployment in the Nomad UI showed. The "Allocations" section at the bottom should show 4 running and 6 stopped allocations. The latter are from when you did the rolling and blue/green updates in the last two challenges.

    If you refresh the "Chat 1", "Chat 2", and "Chat 3" tabs, you will see the app with both the light and the dark backgrounds. However, the dark background should only show up 1/4 of the time. That makes sense since you are running 3 instances of the chat app with the light background from the last challenge along with the single canary that uses the dark background.

    Let's assume that you like the light background of the app better than the dark background that the canary is using and want to roll back the deployment.

    Do that by running this command on the "Server" tab:<br>
    `
    nomad deployment fail <deployment_id>
    `<br>
    replacing <deployment_id\> with the deployment ID of the latest deployment returned by the job status command.

    Nomad returns a message like this:<br>
    `
    Deployment "fa80df0c-0ba8-327a-3ad3-a8e383fe64e1" failed
    ==> Monitoring evaluation "0c2e95f1"
        Evaluation triggered by job "chat-app"
        Evaluation within deployment: "fa80df0c"
        Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "0c2e95f1" finished with status "complete"
    `<br>
    This indicates that Nomad has failed the deployment. The Nomad UI will also show a message "Deployment marked as failed".

    What the message does not say is that Nomad will stop the canary allocation and leave the 3 allocations with the light background running. However, you can confirm that this has happened by refreshing the Chat tabs multiple times. You will only see the light background.

    Together, this challenge and the previous one show that you can test out changes to an application with Nomad and then either promote the changes or roll them back. The choice is yours.

    Congratulations on completing the Nomad Job Update Strategies track!
  notes:
  - type: text
    contents: |-
      In this challenge, you will do a "canary" deployment of the Chat job for which Nomad will deploy 1 new "canary" instance of a Docker-based version of the chat app with the dark background but keep the 3 instances with the light background that you deployed in the last challenge running.

      However, instead of promoting the deployment, you will roll it back.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 8080
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 8080
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 8080
  difficulty: basic
  timelimit: 600
checksum: "482772090237461827"
