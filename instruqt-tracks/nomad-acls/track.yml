slug: nomad-acls
id: vlqq3najdjpp
type: track
title: Nomad Access Control Lists (ACLs)
teaser: Learn how to control access to Nomad with Access Control Lists (ACLs).
description: |-
  Nomad is a flexible workload orchestrator that enables an organization to easily deploy and manage any containerized or legacy application using a single, unified workflow. Nomad can run a diverse workload of Docker, non-containerized, microservice, and batch applications.

  This track will show you how to control access to Nomad with Access Control Lists (ACLs).

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics), [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster), and [Nomad Multi-Server Cluster](https://instruqt.com/hashicorp/tracks/nomad-multi-server-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- nomad
- security
- acls
owner: hashicorp
developers:
- lhaig@hashicorp.com
- roger@hashicorp.com
private: true
published: true
challenges:
- slug: run-the-servers-and-clients
  id: 5efmxtpkfxlm
  type: challenge
  title: Confirm the Agents are running.
  teaser: Confirming that the Agents are configured and running.
  assignment: |-
    Before we begin securing our Nomad cluster, lets verify that all 3 servers and both clients are running.

    Run all the following commands on the 'Server 1' tab.

    Check the servers:
    ```
    nomad server members
    ```
    You should see 3 servers that are all alive. server1 should be the leader.

    Then check the client nodes:
    ```
    nomad node status
    ```
    You should see two client nodes.

    We have run a job that we will use as part of the challenges to visualize the use of ACLs.

    Check the job status:
    ```
    nomad status
    ```
    You should see a job with the id of `redis`.

    In the next challenge, you will configure ACLs for the Nomad cluster.
  notes:
  - type: text
    contents: |-
      In this track, you'll learn how to control access to Nomad with Access Control Lists (ACLs).

      The first challenge will run 3 Nomad servers and 2 Nomad clients for you. All you'll need to do is check that they are running.

      In the following challenges, you will configure and then use Nomad ACLs.

      We have run a job called `redis` which we will be using as part of our challenges as we move on.
  tabs:
  - title: Server 1
    type: terminal
    hostname: nomad-server-1
  - title: Server 2
    type: terminal
    hostname: nomad-server-2
  - title: Server 3
    type: terminal
    hostname: nomad-server-3
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 1800
- slug: configure-server-acls
  id: rfb3dvxmxuyp
  type: challenge
  title: Configure Nomad Server ACLs
  teaser: Configure Nomad Server ACLs.
  assignment: |-
    We need to enable the ACL system for our cluster.

    Let's start by configuring ACLs on our Nomad servers.

    To do this, we need to enable ACLs in the `acl` stanza of each server file. Go to each "Server Config" tab and add the following section to the end of the file, below the server stanza.
    ```
    acl {
      enabled = true
    }
    ```
    After adding copying and pasting the text from the assignment, click the disk icon to save your changes.

    In a multi-region topology you would need to add the "authoritative_region" configuration to the server stanza. Since we only have one region, this is not strictly necessary. We will, however, make the change to show how it should look.

    For each server, open the "Server Config" tab and edit the `server` stanza by adding<br>
    `authoritative_region = "global"`<br>
    under the line<br>
    `bootstrap_expect = 3`.

    Now we need to restart the nomad servers one at a time. Run the following command for each server on the server console tabs.
    ```
    systemctl restart nomad
    ```

    Please run the next command on the "Server 1" tab:
    ```
    nomad server members
    ```
    What is the result?

    If everything was configured correctly, you will get a "403 permission denied" error.  This is the expected behavior since the Nomad ACL system is "Deny by default". We will fix this in a later challenge.

    You will not be able to query the job status either.

    To make sure the `nomad` process is running, we can run the following command on each server console tab:
    ```
    ps -ef | grep nomad
    ```

    The result should be similar to this on each server.
    `
    root@nomad-server-1:~# ps -ef | grep nomad
    root      2203     1  1 10:28 ?        00:01:25 /usr/local/bin/nomad agent -config /etc/nomad.d
    root      3369  3345  0 12:06 pts/0    00:00:00 grep nomad
    root@nomad-server-1:~#
    `

    If you see two lines for all 3 servers, it confirms that the Nomad servers are all running.

    In the next challenge, you will configure the Nomad clients for utilizing ACLs.
  notes:
  - type: text
    contents: |-
      In this challenge, you will configure the Nomad servers to use ACLs.

      In the next challenge, you will configure the Nomad clients to use ACLs.
  tabs:
  - title: Server 1 Config
    type: code
    hostname: nomad-server-1
    path: /etc/nomad.d/server1.hcl
  - title: Server 1
    type: terminal
    hostname: nomad-server-1
  - title: Server 2 Config
    type: code
    hostname: nomad-server-2
    path: /etc/nomad.d/server2.hcl
  - title: Server 2
    type: terminal
    hostname: nomad-server-2
  - title: Server 3 Config
    type: code
    hostname: nomad-server-3
    path: /etc/nomad.d/server3.hcl
  - title: Server 3
    type: terminal
    hostname: nomad-server-3
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 1800
- slug: configure-client-acls
  id: ncufcvo3ycxz
  type: challenge
  title: Configure Nomad Client ACLs
  teaser: Configure Nomad Client ACLs.
  assignment: |-
    Now that we have the servers using ACLs, we need to configure the Clients to use them as well.

    We need to enable ACLs in the `acl` stanza of each client file. Go to each Client Config tab and add the following section to the end of the file, below the client stanza.
    ```
    acl {
      enabled = true
    }
    ```
    That is all configuration that needs to be done.

    Now, you need to restart the nomad clients one at a time as you did for the servers. Run the following command for each client:
    ```
    systemctl restart nomad
    ```
    After you have restarted the Nomad Agents.

    Run the following command on the "Client 1" tab:
    ```
    nomad node status
    ```
    What is the result?

    You will see the same error that you received when you tried to check the server status earlier.

    Run the same process check command you ran in the previous challenge on both client node consoles to make sure that the Nomad process is running on both clients.
    ```
    ps -ef | grep nomad
    ```

    The result should be similar to this on each client:
    `
    root@nomad-client-1:~# ps -ef | grep nomad
    root      2203     1  1 10:28 ?        00:01:25 /usr/local/bin/nomad agent -config /etc/nomad.d
    root      3369  3345  0 12:06 pts/0    00:00:00 grep nomad
    root@nomad-client-1:~#
    `<br>
    Note, however, that one client will have some extra Nomad processes because of the running job.

    If you do see at least one Nomad process on each client (other than the `grep` command), that confirms that the agents are running.

    In the next challenge, we will bootstrap the cluster ACL system and create our first management token.
  notes:
  - type: text
    contents: |-
      In this challenge, you will configure the Nomad clients to use ACLs.

      In the next challenge, you will bootstrap the Nomad cluster's ACL system.
  tabs:
  - title: Client 1 Config
    type: code
    hostname: nomad-client-1
    path: /etc/nomad.d/client1.hcl
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2 Config
    type: code
    hostname: nomad-client-2
    path: /etc/nomad.d/client2.hcl
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 1800
- slug: bootstrap-acls
  id: agli9we12h2m
  type: challenge
  title: Bootstrap Nomad ACLs
  teaser: Bootstrap Nomad ACLs.
  assignment: |-
    Now that we have the servers and clients using ACLs, we need to bootstrap the ACL system.

    To enable the ACL system, we need to run the `nomad bootstrap` command. This will create the initial management ACL token for us to use to set up other parts of the ACL system.

    This token should be kept in a safe place since it has **FULL UNRESTRICTED** access to your cluster.

    Let's bootstrap the ACL system on the cluster.

    Open the "Server 1" tab and run the following command:
    ```
    cd nomad
    nomad acl bootstrap > acl_token.txt
    ```

    This will bootstrap the cluster's ACL system and export the bootstrap credentials into a file called "acl_token.txt", which we will be using to continue the challenge.

    We now need to enable the `anonymous` ACL policy so that we can interact with the cluster using the Nomad CLI.

    To do this, we need to export the "Secret ID" of our token as an environment variable called `NOMAD_TOKEN` and then use `curl` to create the policy.

    You can however use both `curl` and the `nomad` cli to add policies to the cluster. We will initially use curl to add the anonymous policy and then move to the nomad cli for the next challenge.
    Policy file rules can either be created using JSON syntax or using the HCL syntax.

    Open the "Files" tab and you will see two files:
      * anonymous.json
      * acl_token.txt

    Open the anonymous.json file.
    It should look like this.
    ```
    {
      "Name": "anonymous",
      "Description": "Allow read-only access for anonymous requests",
      "Rules": "
        namespace \"default\" {
          policy = \"read\"
        }
        agent {
          policy = \"read\"
        }
        node {
          policy = \"read\"
        }
      "
    }

    ```

    As you can see, this policy allows read access for three rules
      * namespace
      * agent
      * node

    We now need to apply this policy to the cluster. To do this, we will need to create an "authenticated" connection to the server that allows us to install policies.

    Open the acl_token.txt file. It should look something like this:
    ```
    Accessor ID  = 6fa2c331-a156-9238-ed2d-fed666518934
    Secret ID    = 187abe9e-abb5-a917-5176-99d7eb642b44
    Name         = Bootstrap Token
    Type         = management
    Global       = true
    Policies     = n/a
    Create Time  = 2020-01-09 17:58:06.854257747 +0000 UTC
    Create Index = 29
    Modify Index = 29
    ```

    Copy the "Secret ID" token.

    Run **ALL** of the following commands on the "Server 1" console tab:<br>
    `export NOMAD_TOKEN=<Secret ID>`<br>
    where <Secret ID/> is your "Secret ID" token.

    We now need to install the policy. Please run the following `curl` command:
    ```
    curl --request POST --data @anonymous.json -H "X-Nomad-Token: $NOMAD_TOKEN" http://localhost:4646/v1/acl/policy/anonymous
    ```

    To verify that the policy has been applied, run the following command:
    ```
    curl http://localhost:4646/v1/jobs | jq
    ```
    It should not give an error and should give nicely formatted JSON since we used `jq`.

    Now we should be able to run our CLI commands from before.

    Check the state of the servers:
    ```
    nomad server members
    ```
    You should see the 3 servers that are all alive. One of the servers will be a leader.

    Check the node status:
    ```
    nomad node status
    ```
    You should see the 2 client servers.

    In the next challenge, you will see how the Nomad ACLs restrict what different users can do in Nomad.
  notes:
  - type: text
    contents: |-
      In this challenge, you will bootstrap the cluster to use ACLs.

      The ACL system uses a whitelist or default-deny model. This means no permissions are initially granted.

      For clients making requests without ACL tokens, we might want to grant some basic level of access. So, we will create an anonymous ACL policy.

      In the final challenge, you will see how the ACLs restrict what different users can do in Nomad.
  tabs:
  - title: Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server 1
    type: terminal
    hostname: nomad-server-1
  - title: Server 2
    type: terminal
    hostname: nomad-server-2
  - title: Server 3
    type: terminal
    hostname: nomad-server-3
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 1800
- slug: use-acls
  id: 2dzqorlvompb
  type: challenge
  title: Using Nomad ACLs
  teaser: See how Nomad ACLs restrict what different users can do in Nomad.
  assignment: |-
    Now, let's see the Nomad ACLs in action.

    In the last challenge, we created an anonymous ACL policy that allows any anonymous unauthenticated connection to have read only access to the the default namespace, agents, and nodes.

    To confirm we still have this access, open the "Server 2" tab and run the following commands.
    ```
    nomad job status
    ```
    You should see that the "redis" job is running.

    Then run:
    ```
    nomad agent-info
    ```
    You should see all the information about the local Nomad agent.

    This is not an ideal situation as anyone with access to the CLI or HTTP API could get quite a bit of information about the cluster and its jobs.

    Let's remove this access to make our cluster more secure.

    In the "Server 1" tab, run the following commands:
    ```
    cd nomad
    export NOMAD_TOKEN=`cat acl_token.txt | grep Secret | awk '{print$4}'`
    ```
    We will now delete the anonymous policy:
    ```
    nomad acl policy delete anonymous
    ```

    Once this has run, go back to the "Server 2" tab and try to run this:
    ```
    nomad agent-info
    ```
    You should receive a "Permission denied" error.

    We will now create 2 ACL policies, one for our dev team and one for our ops team.

    For the purpose of this example, we will prevent the dev team from stopping any jobs.

    In the "Config Files" tab you will now see 2 new files:
      * dev_policy.hcl
      * ops_policy.hcl

    These two policy files will give the dev and ops teams different permissions in the cluster.

    The dev team will get read only access to namespace APIs which are the jobs, deployments, allocations, and evaluations APIs. The ops team will get full write access to those APIs.

    Let's apply the policies.

    We will use the Nomad CLI to apply the policies to the cluster.

    Run the following commands on the "Server 1" tab.

    To apply the dev policy run the following command.
    ```
    nomad acl policy apply -description "Dev Read Only" devRO dev_policy.hcl
    ```
    We have named the policy `devRO`.

    To apply the ops policy, run:
    ```
    nomad acl policy apply -description "OPS Read write" opsRW ops_policy.hcl
    ```
    We have named the policy `opsRW`.

    Run the following command to see a list of the ACL policies that are now applied.

    ```
    nomad acl policy list
    ```

    The list should have the 2 policies we just applied.

    To use these policies we need to create some ACL tokens for the dev and ops team.

    It is very easy to create these tokens using the Nomad CLI.

    Run the following commands on the "Server 1" tab:
    ```
    nomad acl token create -name="Dev RO" -type="client" -policy="devRO" > devro_token.txt
    ```

    This will create our `Dev RO` read only token and apply the devRO policy to this token. We export the token to a text file so we can use it later.

    We will do the same for the Ops token by running the following command.
    ```
    nomad acl token create -name="Ops RW" -type="client" -policy="opsRW" > opsrw_token.txt
    ```

    If you now run
    ```
    nomad acl token list
    ```
    you should see a list with the two new tokens. You will also see the main bootstrap token we are using.

    We can now test the access of the team with their tokens.

    On the "Server 2" tab, export the `Dev RO` token with this command:<br>
    `export NOMAD_TOKEN=<dev_token>`<br>
    replacing <dev_token/> with the value of `Secret ID` in the devro_token.txt file on the "Config Files" tab.

    If you now run `nomad job status`, you should see the `redis` job listed again.  But, if you try to run `nomad job stop redis`, you should get a "Permission denied" error.

    Now let's test the access of the ops team:

    In the "Server 3" tab, export the `Ops RW` token with this command:<br>
    `export NOMAD_TOKEN=<ops_token>`<br>
    replacing <ops_token/> with the value of `Secret ID` in the opsrw_token.txt file on the "Config Files" tab.

    If you now run `nomad job status`, you should see the `redis` job listed again.

    If you try to run `nomad job stop redis`, the job should be stopped.

    If you try to run `cd nomad` followed by `nomad job run redis.nomad`, the job should be started again.

    As you can see, the ACL permissions can be adjusted in many ways.  We recommended that you read up further about the policies in the [Nomad Access Control](https://www.nomadproject.io/guides/security/acl.html) guide.

    Congratulations on finishing the Nomad ACLs track!
  notes:
  - type: text
    contents: |-
      In this final challenge, you will see how the ACLs restrict what different users can do in Nomad.

      We will apply a Developer policy and also an Operations policy to the cluster using the Nomad CLI.
  - type: text
    contents: |-
      We will create two client tokens, one for a Developer role and one for an Operations role. We will also assign these to the corresponding policies.

      We will then use these tokens to perform some basic actions on the cluster and see that only members of the Operations role can start and stop jobs.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server 1
    type: terminal
    hostname: nomad-server-1
  - title: Server 2
    type: terminal
    hostname: nomad-server-2
  - title: Server 3
    type: terminal
    hostname: nomad-server-3
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 1800
checksum: "10817813741932245752"
