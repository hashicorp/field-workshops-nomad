# Copyright (c) HashiCorp, Inc.
# SPDX-License-Identifier: MPL-2.0

slug: nomad-consul-connect
id: juyvie8yv7ky
type: track
title: Nomad Integration with Consul Connect
teaser: |
  Learn how to run Nomad jobs secured by Consul Connect.
description: |-
  This track will show you how easy it is to register Nomad tasks as Consul services so that they can talk to each other using Consul's service discovery and how you can use Nomad's integration with [Consul Connect](https://www.nomadproject.io/docs/integrations/consul-connect/) to secure the communications between the services.

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics) and [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- nomad
- consul
- consul connect
owner: hashicorp
developers:
- roger@hashicorp.com
private: true
published: true
show_timer: true
challenges:
- slug: verify-nomad-cluster-health
  id: y96veofixdxn
  type: challenge
  title: Verify the Health of Your Nomad Cluster
  teaser: |
    Verify the health of the Nomad cluster that has been deployed for you.
  notes:
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts.

      This will include checking the health of a Consul cluster that has been set up on the same VMs.
  assignment: |-
    In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 2 Nomad/Consul clients. They are using Nomad 1.0.0 and Consul 1.9.0.

    First, verify that all 3 Consul agents are running and connected to the cluster by running this command on the "Server" tab:
    ```
    consul members
    ```
    You should see 3 Consul agents with the "alive" status.

    Check that the Nomad server is running by running this command on the "Server" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server with the "alive" status.

    Check the status of the Nomad client nodes by running this command on the "Server" tab:
    ```
    nomad node status
    ```
    You should see 2 Nomad clients with the "ready" status.

    You can also check the status of the Nomad server and clients in the Nomad and Consul UIs.

    In the next challenge, you will run a job that uses Nomad's native integrations with Consul and Consul Connect.
  tabs:
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 900
- slug: nomad-and-consul-connect
  id: qtu9dyfttayl
  type: challenge
  title: Nomad's Integration with Consul Connect
  teaser: |
    Run a Nomad job with tasks that communicate securely using Consul Connect.
  notes:
  - type: text
    contents: |-
      In this challenge, you will run a job that uses Nomad's native integration with Consul. The job's tasks will use Consul's service registration, service discovery, and secure service-to-service communication (using Consul Connect).

      This challenge is based on the  Consul Connect](https://www.nomadproject.io/docs/integrations/consul-connect/) in the Nomad documentation.

      There is also a [version](https://learn.hashicorp.com/tutorials/nomad/consul-service-mesh) of this guide that can be used with a Consul cluster that has Consul ACLs enabled.
  assignment: |-
    In this challenge, you will run a job that uses Nomad's native integrations with Consul and Consul Connect. The job's tasks will use Consul's service registration, service discovery, and secure service-to-service communication (using Consul Connect).

    In order to support Consul Connect, Nomad adds a new networking mode for jobs that enables tasks in the same task group to share their networking stack. When Connect is enabled, Nomad launches an Envoy proxy alongside the application in the job file. The proxy (Envoy) provides secure communication with other applications in the Nomad cluster.

    First, examine the Nomad job specification file, connect.nomad that we have placed in the /root/nomad directory. You can use the code editor on the "Config Files" tab.

    This job specification file defines two task groups:
      1. The `api` task group runs a counting service API in a container running the `hashicorpnomad/counter-api` Docker image on port 9001 on the bridge network.
      2. The `dashboard` task group runs a web dashboard  in a container running the `hashicorpnomad/counter-dashboard` Docker image on port 9002 on the bridge network.

    The `api` task group registers itself with Consul as the `count-api` service listening on port 9001. It runs on the bridge network and therefore has an isolated network namespace with an interface bridged to the host it runs on. It does not define any ports in its network because it is only accessible via Consul Connect. The Envoy proxy will automatically route traffic sent to the service using port 9001 to the port inside the network namespace dynamically selected by Nomad.

    The `dashboard` task group registers itself with Consul as the `count-dashboard` service. It runs on the bridge network and uses the static forwarded port, 9002, which asks Nomad to use external port 9002 for the service and to forward requests to the same port inside the task group's own network namespace.

    So, a web browser can connect to the dashboard using `http://<host_ip>:9002`. The dashboard uses Consul Connect to make calls to the api service through a sidecar proxy. The `upstreams` stanza in the `proxy` stanza indicates that the sidecar proxy will listen on port 8080 inside the dashboard task group's network namespace. This means that requests to the dashboard on port 9002 will be forwarded to the sidecar proxy on port 8080.

    The `dashboard` task within the `dashboard` task group uses the `COUNTING_SERVICE_URL` environment variable set to `http://${NOMAD_UPSTREAM_ADDR_count_api}`. This uses Nomad's interpolation to tell the dashboard application inside the Docker container what dynamic IP and port to use to talk to the sidecar proxy.

    Summarizing, Nomad will automatically deploy two Consul Connect proxies to allow the dashboard web application to communicate securely with the count-api service. Consul Connect tells the dashboard proxy how to find the count-api proxy.

    Now, that we've explored the connect.nomad job specification, you can run the job.  Please do this on the "Server 1" tab by running:
    ```
    cd nomad
    nomad job run connect.nomad
    ```
    You should see something like this:
    ```
    ==> Monitoring evaluation "fdf1c932"
    Evaluation triggered by job "countdash"
    Evaluation within deployment: "ab006141"
    Allocation "8dff276a" created: node "f94fc4ad", group "api"
    Allocation "f4225a5f" created: node "f94fc4ad", group "dashboard"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "fdf1c932" finished with status "complete"
    ```

    Finally, use the dashboard web application on the "Dashboard 1" or the "Dashboard 2" tab.  We have exposed tabs for both of the Nomad clients because we don't know in advance which client Nomad will schedule the task groups on.

    As you watch the app, you should see the counter climb.

    You can also look at the tasks associated proxies in both the Nomad and the Consul UIs.

    Congratulations on completing the Nomad Integration with Consul Connect track!
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server 1
    type: terminal
    hostname: nomad-server-1
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Dashboard-1
    type: service
    hostname: nomad-client-1
    port: 9002
  - title: Dashboard-2
    type: service
    hostname: nomad-client-2
    port: 9002
  difficulty: basic
  timelimit: 2700
checksum: "13817559568933083282"
