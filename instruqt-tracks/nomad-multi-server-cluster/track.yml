slug: nomad-multi-server-cluster
id: ygythoighyka
type: track
title: Nomad Multi-Server Cluster
teaser: |
  Learn how to bootstrap multi-server Nomad clusters in two different ways.
description: |-
  This track will show you how to bootstrap a Nomad cluster that has three servers and two clients in two different ways. You'll first use Nomad's manual clustering and then use automatic clustering with Consul.

  You'll also see how easy it is to register Nomad tasks as Consul services so that they can talk to each other using Consul's service discovery.

  In fact, we'll even use Nomad's integration with [Consul Connect](https://www.nomadproject.io/docs/integrations/consul-connect/) to secure the communications between the services!

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics) and [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- nomad
- cluster
- consul
owner: hashicorp
developers:
- pgryzan@hashicorp.com
- roger@hashicorp.com
private: true
published: true
show_timer: true
challenges:
- slug: manual-clustering
  id: 7kmuc2a1yqge
  type: challenge
  title: Bootstrap a Nomad Cluster Manually
  teaser: |
    Bootstrap a Nomad cluster with 3 servers and 2 Nomad clients using manual clustering.
  assignment: |-
    In this challenge, you will bootstrap a Nomad cluster with 3 servers and 2 Nomad clients using manual clustering.

    Let's start by looking at the `nomad-server1.hcl` configuration file on the "Config Files" tab on the left. You'll see that we are calling this server node `server1`, configuring the agent as a server, and indicating that we expect three servers in the cluster with the `bootstrap_expect` setting.

    Next, start server1 in the background on the "Server 1" tab by running:
    ```
    cd nomad
    nomad agent -config nomad-server1.hcl > nomad.log 2>&1 &
    ```
    You should see a single line like `[1] 2174`, which gives the PID of the Nomad server.

    We suggest you also tail the nomad.log file by running `tail -f nomad.log`. This will show many messages including one that says "Nomad agent started!". It will also show some error messages saying "No cluster leader". That is expected at this point because we have not yet strated 3 servers.

    Now, look at the `nomad-server2.hcl` configuration file on the "Config Files" tab. Note that it is is similar to `nomad-server1.hcl` but has a `server_join` stanza indicating that server2 should repeatedly try to join server1 until it succeeds.

    Start the second Nomad server on the "Server 2" tab, specifing `nomad-server2.hcl` as the configuration file:
    ```
    cd nomad
    nomad agent -config nomad-server2.hcl > nomad.log 2>&1 &
    ```

    At this point, if you are still tailing the nomad.log for server1, you should see a message like "nomad: serf: EventMemberJoin: server2.global".

    If you look at the `nomad-server3.hcl` configuration file on the "Config Files" tab, you'll see that it also includes a `server_join` stanza indicating that server3 should repeatedly try to join server1 until it succeeds.

    Start the third Nomad server on the "Server 3" tab, specifing `nomad-server3.hcl` as the configuration file:
    ```
    cd nomad
    nomad agent -config nomad-server3.hcl > nomad.log 2>&1 &
    ```

    At this point, if you are still tailing the nomad.log for server1, you should see messages indicating that server3 joined the cluster and that server1 has been elected the cluster's leader.

    To double-check that all 3 servers have joined the cluster, run this command on the "Server 3" tab:
    ```
    nomad server members
    ```
    You should see output listing all 3 server nodes, all 3 of which should be considered "alive". server1 should be considered the leader of the cluster.

    If clicking the Check button at the end of the challenge says you have not done this, you might have run it on one of the other tabs. Please run it again on the "Server 3" tab so that it will end up in your .bash_history file on the nomad-server-3 VM.

    Now, look at the `nomad-client1.hcl` configuration file on the "Config Files" tab. This file indicates that the agent will run as a client node and connect to the `nomad-server-1` server.

    Next, run the first Nomad client on the "Client 1" tab:
    ```
    cd nomad
    nomad agent -config nomad-client1.hcl > nomad.log 2>&1 &
    ```
    You should see a single line like `[1] 2241`, which gives the PID of the first client. You can inspect the nomad.log file by running `cat nomad.log`. This will show many messages including one that says "Nomad agent started!"

    Now, look at the `nomad-client2.hcl` configuration file on the "Config Files" tab. It is very similar to the `nomad-client1.hcl` file.

    Start the second Nomad client on the "Client 2" tab, but specify `nomad-client2.hcl` as the configuration file:
    ```
    cd nomad
    nomad agent -config nomad-client2.hcl > nomad.log 2>&1 &
    ```
    You should see a single line like `[1] 2065`, which gives the PID of the second client. As with the first client, you can inspect the nomad.log file by running `cat nomad.log`.

    Check the status of the Nomad client nodes on the "Server 3" tab:
    ```
    nomad node status
    ```
    You should see two client nodes. (The command only shows client nodes, so the servers are not listed.)

    If clicking the Check button at the end of the challenge says you have not done this, you might have run it on one of the other tabs. Please run it again on the "Server 3" tab so that it will end up in your .bash_history file on the nomad-server-3 VM.

    You can now inspect the servers and clients in the Nomad UI. You might initially see a "Server Error" message in the UI saying that "A server error prevented data from being sent to the client".  Just click on the "Go to Clients" button. You should then see the UI and can inspect the servers and clients that you started.

    Of course, you won't see any jobs yet.
  notes:
  - type: text
    contents: |-
      This track will show you how to bootstrap a Nomad cluster that has three servers and two clients in two different ways. You will also learn how easy it is to register Nomad tasks as Consul services and secure them with Nomad's native integration with [Consul Connect](https://www.nomadproject.io/docs/integrations/consul-connect/).

      In this first challenge, you will bootstrap the Nomad cluster using manual clustering.

      In the next challenge, you will use automatic clustering with Consul.
  - type: text
    contents: |-
      We've put all the server and client configuration files on the server1 VM so that you can conveniently see them in a single Instruqt tab.

      However, if you decide to edit any of the configuration files for any reason, be sure to do so on the tab that matches the file you change.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server 1
    type: terminal
    hostname: nomad-server-1
  - title: Server 2
    type: terminal
    hostname: nomad-server-2
  - title: Server 3
    type: terminal
    hostname: nomad-server-3
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 1200
- slug: automatic-clustering-with-consul
  id: oj8xnvwnobv0
  type: challenge
  title: Bootstrap a Nomad Cluster with Consul
  teaser: Bootstrap a Nomad Cluster with 3 servers and 2 Nomad clients using automated
    clustering with Consul.
  assignment: |-
    In this challenge, you will configure and run a Nomad cluster using automated clustering with Consul.

    Let's start by looking at the `consul-server1.json` configuration file on the "Config Files" tab on the left. This file indicates that the Consul agent will run as a server on this VM and bind to the `ens4` network interface. Additionally, it expects a total of 3 Consul servers to run and will repeatedly try to join to the Consul servers running on the `nomad-server-2` and `nomad-server-3` VMs.

    We have also enabled Consul Connect since we will use it in the next challenge and set the GRPC port to 8502 as required by Consul Connect.

    Next, start the `server1` Consul server in the background on the "Server 1" tab by running:
    ```
    cd nomad
    consul agent -config-file consul-server1.json > consul.log 2>&1 &
    ```
    You should see a single line like `[1] 3194`, which gives the PID of the Consul server.

    Now look at the `consul-server2.json` configuration file on the "Config Files" tab on the left. This file is very similar to `consul-server1.json` but will make the the second Consul server repeatedly try to join to the Consul servers running on the `nomad-server-1` and `nomad-server-3` VMs.

    Next, start the `server2` Consul server in the background on the "Server 2" tab by running:
    ```
    cd nomad
    consul agent -config-file consul-server2.json > consul.log 2>&1 &
    ```
    You should see a single line like `[1] 3196`, which gives the PID of the Consul server.

    Now look at the `consul-server3.json` configuration file on the "Config Files" tab on the left. This file is very similar to `consul-server1.json` but will make the the third Consul server repeatedly try to join to the Consul servers running on the `nomad-server-1` and `nomad-server-2` VMs.

    Next, start the `server3` Consul server in the background on the "Server 3" tab by running:
    ```
    cd nomad
    consul agent -config-file consul-server3.json > consul.log 2>&1 &
    ```
    You should see a single line like `[1] 3198`, which gives the PID of the Consul server.

    Now look at the `consul-client1.json` configuration file on the "Config Files" tab on the left. This file is very similar to `consul-server1.json` but does not include the `server` stanza and will make the the Consul client repeatedly try to join to all 3 Consul servers.

    Next, start the `client1` Consul client in the background on the "Client 1" tab by running:
    ```
    cd nomad
    consul agent -config-file consul-client1.json > consul.log 2>&1 &
    ```
    You should see a single line like `[1] 3198`, which gives the PID of the Consul client.

    If you look at the `consul-client2.json` configuration file on the "Config Files" tab, you will see that is very similar to the `consul-client1` file.

    Start the `client2` Consul client in the background on the "Client 2" tab by running:
    ```
    cd nomad
    consul agent -config-file consul-client2.json > consul.log 2>&1 &
    ```
    You should see a single line like `[1] 3198`, which gives the PID of the Consul client.

    Next, verify that all 5 Consul agents are running and connected to the cluster by running the following command on the "Server 3" tab:
    ```
    consul members
    ```
    You should see 3 servers and 2 clients listed. All should be "alive".

    Now that the Consul cluster is running, we can move on to restarting the Nomad agents.

    First, look at the `nomad-server1.hcl` configuration file on the "Config Files" tab on the left. This file is the same as it was for the last challenge.

    Next, start the `server1` Nomad server in the background on the "Server 1" tab by running:
    ```
    nomad agent -config nomad-server1.hcl > nomad.log 2>&1 &
    ```
    You should see a single line like `[1] 2174`, which gives the PID of the Nomad server.

    We suggest you also tail the nomad.log file by running `tail -f nomad.log`. This will show many messages including one that says "Nomad agent started!". It will also show some error messages saying "No cluster leader". That is expected at this point because we have not yet strated 3 servers.  Note that depending on how quickly this command was run, the "Noamd agent started" entry may have already scrolled.

    Now look at the `nomad-server2.hcl` configuration file on the "Config Files" tab. Note that it no longer has the `server_join` stanza that it had in the last challenge; it is no longer needed since the Nomad agents will automatically register themselves with Consul and discover each other.

    Start the second Nomad server on the "Server 2" tab, specifing `nomad-server2.hcl` as the configuration file:
    ```
    nomad agent -config nomad-server2.hcl > nomad.log 2>&1 &
    ```

    At this point, if you are still tailing the nomad.log for server1, you should see a message like "nomad: serf: EventMemberJoin: server2.global".

    If you look at the `nomad-server3.hcl` configuration file on the "Config Files" tab, you'll see that we have also removed the `server_join` stanza from it.

    Start the third Nomad server on the "Server 3" tab, specifing `nomad-server3.hcl` as the configuration file:
    ```
    nomad agent -config nomad-server3.hcl > nomad.log 2>&1 &
    ```

    At this point, if you are still tailing the nomad.log for server1, you should see messages indicating that server3 joined the cluster and that server1 has been elected the cluster's leader.

    To double-check that all 3 servers have joined the cluster, run this command on the "Server 3" tab:
    ```
    nomad server members
    ```
    You should see output listing all 3 server nodes, all 3 of which should be considered "alive", and server1 should be considered the leader of the cluster.

    If clicking the Check button at the end of the challenge says you have not done this, you might have run it on one of the other tabs. Please run it again on the "Server 3" tab so that it will end up in your .bash_history file on the nomad-server-3 VM.

    Now look at the `nomad-client1.hcl` configuration file on the "Config Files" tab. This file indicates that the agent will run as a client node. Similar to the server nodes, the connection to the servers is facilitated by Consul.

    Next, run the first Nomad client on the "Client 1" tab:
    ```
    nomad agent -config nomad-client1.hcl > nomad.log 2>&1 &
    ```
    You should see a single line like `[1] 2241`, which gives the PID of the first client. You can inspect the nomad.log file by running `cat nomad.log`. This will show many messages including one that says "Nomad agent started!"

    Now look at the `nomad-client2.hcl` configuration file on the "Config Files" tab. It is very similar to the `nomad-client1.hcl` file.

    Start the second Nomad client on the "Client 2" tab, but specify `nomad-client2.hcl` as the configuration file:
    ```
    nomad agent -config nomad-client2.hcl > nomad.log 2>&1 &
    ```
    You should see a single line like `[1] 2065`, which gives the PID of the second client. As with the first client, you can inspect the nomad.log file by running `cat nomad.log`.

    Check the status of the Nomad client nodes on the "Server 3" tab:
    ```
    nomad node status
    ```
    You should see two client nodes. (The command only shows client nodes, so the servers are not listed.)

    If clicking the Check button at the end of the challenge says you have not done this, you might have run it on one of the other tabs. Please run it again on the "Server 3" tab so that it will end up in your .bash_history file on the nomad-server-3 VM.

    You can now inspect the servers and clients in the Nomad UI. You might initially see a "Server Error" message in the UI saying that "A server error prevented data from being sent to the client".  Just click on the "Go to Clients" button. You should then see the UI and can inspect the servers and clients that you started.

    Of course, you won't see any jobs yet. You will start one that uses Consul for service discovery and secure communication in the next challenge.
  notes:
  - type: text
    contents: |-
      In this challenge, you will bootstrap the Nomad cluster using automatic clustering with Consul.

      The setup scripts for this challenge have stopped the Nomad processes that were running in the first challenge, deleted their data, updated the Nomad configuration files, and added Consul configuration files.

      We are running the Consul servers on the same VMs as the Nomad servers.
  - type: text
    contents: |-
      We've put all the Consul and Nomad server and client configuration files on the server1 VM so that you can conveniently see them in a single Instruqt tab.

      However, if you decide to edit any of the configuration files for any reason, be sure to do so on the tab that matches the file you change.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server 1
    type: terminal
    hostname: nomad-server-1
  - title: Server 2
    type: terminal
    hostname: nomad-server-2
  - title: Server 3
    type: terminal
    hostname: nomad-server-3
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 1200
- slug: nomad-and-consul-connect
  id: x4iw5kvgywyx
  type: challenge
  title: Nomad's Integration with Consul and Consul Connect
  teaser: Run a Nomad job with tasks that find each other using Consul's service discovery
    and communicate securely using Consul Connect.
  assignment: |-
    In this challenge, you will run a job that uses Nomad's native integrations with Consul and Consul Connect. The job's tasks will use Consul's service registration, service discovery, and secure service-to-service communication (using Consul Connect).

    In order to support Consul Connect, Nomad adds a new networking mode for jobs that enables tasks in the same task group to share their networking stack. When Connect is enabled, Nomad launches an Envoy proxy alongside the application in the job file. The proxy (Envoy) provides secure communication with other applications in the Nomad cluster.

    First, examine the Nomad job specification file, connect.nomad that we have placed in the /root/nomad directory. You can use the code editor on the "Config Files" tab.

    This job specification file defines two task groups:

      1. The `api` task group runs a counting service API in a container running the `hashicorpnomad/counter-api` Docker image on port 9001 on the bridge network.
      2. The `dashboard` task group runs a web dashboard  in a container running the `hashicorpnomad/counter-dashboard` Docker image on port 9002 on the bridge network.

    The `api` task group registers itself with Consul as the `count-api` service listening on port 9001. It runs on the bridge network and therefore has an isolated network namespace with an interface bridged to the host it runs on. It does not define any ports in its network because it is only accessible via Consul Connect. The Envoy proxy will automatically route traffic sent to the service using port 9001 to the port inside the network namespace dynamically selected by Nomad.

    The `dashboard` task group registers itself with Consul as the `count-dashboard` service. It runs on the bridge network and uses the static forwarded port, 9002, which asks Nomad to use external port 9002 for the service and to forward requests to the same port inside the task group's own network namespace.

    With this job, a web browser can connect to the dashboard using `http://<host_ip>:9002`. The dashboard uses Consul Connect to make calls to the api service through a sidecar proxy. The `upstreams` stanza in the `proxy` stanza indicates that the sidecar proxy will listen on port 8080 inside the dashboard task group's network namespace. This means that requests to the dashboard on port 9002 will be forwarded to the sidecar proxy on port 8080.

    The `dashboard` task within the `dashboard` task group uses the `COUNTING_SERVICE_URL` environment variable set to `http://${NOMAD_UPSTREAM_ADDR_count_api}`. This uses Nomad's interpolation to tell the dashboard application inside the Docker container what dynamic IP and port to use to talk to the sidecar proxy.

    Summarizing, Nomad will automatically deploy two Consul Connect proxies to allow the dashboard web application to communicate securely with the count-api service. Consul Connect tells the dashboard proxy how to find the count-api proxy.

    Now, that we've explored the connect.nomad job specification, you can run the job.  Please do this on the "Server 1" tab by running:
    ```
    cd nomad
    nomad job run connect.nomad
    ```
    You should see something like this:
    ```
    ==> Monitoring evaluation "fdf1c932"
    Evaluation triggered by job "countdash"
    Evaluation within deployment: "ab006141"
    Allocation "8dff276a" created: node "f94fc4ad", group "api"
    Allocation "f4225a5f" created: node "f94fc4ad", group "dashboard"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "fdf1c932" finished with status "complete"
    ```

    Finally, use the dashboard web application on the "Dashboard 1" or the "Dashboard 2" tab.  We have exposed tabs for both of the Nomad clients because we don't know in advance which client Nomad will schedule the task groups on.

    As you watch the app, you should see the counter climb.

    You can also look at the tasks associated with proxies in both the Nomad and the Consul UIs.

    Congratulations on completing the Nomad Multi-Server Cluster track!
  notes:
  - type: text
    contents: |-
      In this challenge, you will run a job that uses Nomad's native integration with Consul. The job's tasks will use Consul's service registration, service discovery, and secure service-to-service communication (using Consul Connect).

      This challenge is based on the [Nomad Consul Connect Example](https://www.nomadproject.io/docs/integrations/consul-connect/) in the Nomad documentation.

      For additional information about Consul Connect, see the [Consul Connect](https://www.consul.io/docs/connect/index.html) documentation.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server-1
    path: /root/nomad/
  - title: Server 1
    type: terminal
    hostname: nomad-server-1
  - title: Server 2
    type: terminal
    hostname: nomad-server-2
  - title: Server 3
    type: terminal
    hostname: nomad-server-3
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Dashboard-1
    type: service
    hostname: nomad-client-1
    port: 9002
  - title: Dashboard-2
    type: service
    hostname: nomad-client-2
    port: 9002
  difficulty: basic
  timelimit: 1200
checksum: "10711246043662458330"
