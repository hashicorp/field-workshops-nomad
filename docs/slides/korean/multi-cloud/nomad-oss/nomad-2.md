name: nomad-chapter-2-title
class: title, shelf, no-footer, fullbleed
background-image: url(https://hashicorp.github.io/field-workshops-assets/assets/bkgs/HashiCorp-Title-bkg.jpeg)
count: false

# Chapter 2
## Nomad ì»¨ì…‰ê³¼ êµ¬ì¡°

![:scale 15%](https://hashicorp.github.io/field-workshops-assets/assets/logos/logo_nomad.png)

???
Nomad is a highly advanced service scheduler and manager.  Within this slide deck, we'll be reviewing some of the more advanced concepts and architecture behind Nomad.

---
layout: true

.footer[
- Copyright Â© 2021 HashiCorp
- ![:scale 100%](https://hashicorp.github.io/field-workshops-assets/assets/logos/HashiCorp_Icon_Black.svg)
]

---
name: chapter-2-topics
# Chapter 2 Topics

1. Nomadì˜ ì£¼ìš” ì»¨ì…‰
2. Nomad ì•„í‚¤í…ì²˜
3. Nomad ìŠ¤ì¼€ì¥´ë§
4. Nomad í†µí•©(Integration)

???
* This is our chapter topics slide.

---
name:  What Is Nomad
# Nomad
.smaller[
* ìœ ì—°í•˜ê³  ê°€ë³ê³  ê³ ì„±ëŠ¥ì´ë©° ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
* ì»¨í…Œì´ë„ˆì™€ ë ˆê±°ì‹œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë™ì‹œì— ë°°í¬í•˜ê³  ê´€ë¦¬
* ë°ì´í„° ì„¼í„° ë° í´ë¼ìš°ë“œ í”Œë«í¼ì—ì„œ ì‘ë™í•˜ê³  ë²”ìš© ìŠ¤ì¼€ì¤„ë§ ì œê³µ
* ì„œë¹„ìŠ¤, ë°°ì¹˜ ìŠ¤ì¼€ì¤„ë§ê³¼ ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ ê´€ë¦¬
]

???
- Runs as a single binary in just about any environment - one of the easiest and lightweight service scheduler and manager available.
-  Used to deploy both container applications, as well as legacy applications such as Java or raw executables.
-  As an independent function, Nomad can run and communicate across data centers, and cloud platforms.  Truly cloud agnostic.
-  Can manage individual services, batch functions, or even global system services such as monitoring functions.

---
name:  Common Nomad Concepts
# Nomad Concepts (1)

.smaller[
* **Clusters** : ë‹¨ì¼ Regionì—ì„œ Nomad ë°”ì´ë„ˆë¦¬ë¡œ ì„œë²„ì™€ í´ë¼ì´ì–¸íŠ¸ë¡œ êµ¬ì„±
* **Servers** : í´ëŸ¬ìŠ¤í„° ë‚´ì— ìŠ¤ì¼€ì¤„ë§ê³¼ ì‘ì—… í• ë‹¹ ê°™ì€ ì§€ëŠ¥ì  ì—­í•  ìˆ˜í–‰
* **Clients** : ì„œë²„ì— ë“±ë¡ëœ ì˜ˆì•½ ì‘ì—…ì„ ì‹¤í–‰
* **Jobs** : ì‚¬ìš©ìê°€ ì„œë²„ì— ì œì¶œí•œ ì›Œí¬ë¡œë“œì˜ ì›í•˜ëŠ” ìƒíƒœë¥¼ ì •ì˜
* **Drivers** : Docker, Exec, Java ê°™ì€ ì‘ì—…ì˜ ì •ì˜ë¥¼ ì‹¤í–‰í•˜ëŠ”ë° ì‚¬ìš©
* **Tasks** : Driverê°€ ì‹¤í–‰í•˜ëŠ” ê°€ì¥ ì‘ì€ ë‹¨ìœ„
* **Task Groups** : í•¨ê»˜ ì‹¤í–‰í•˜ëŠ” ì‘ì—… ê·¸ë£¹ìœ¼ë¡œ í•˜ë‚˜ì˜ í´ë¼ì´ì–¸íŠ¸ ë‚´ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë‹¨ìœ„
]

???
-  Cluster contains anywhere from 3-5 server nodes, and an unconstrained number of client nodes.
-  All nodes run the same Nomad binary.
-  Nomad server nodes provide the brains and intelligence to the cluster, performing all scheduling and allocations
-  Clients execute the tasks as directed by the server cluster.
-  Jobs are used to describe the desired state of the workloads.
-  Drivers for docker containers, java apps, and arbitrary executables execute the defined tasks
-  Task Groups are groups of tasks that must be run together, often colocated on the same client -  may be required for various architectural reasons.

---
name:  More Nomad Concepts
# Nomad Concepts (2)

.smaller[
* **Allocations** : Jobì— ì •ì˜ëœ taskì™€ groupì„ í´ë¼ì´ì–¸íŠ¸ì— ë§µí•‘
* **Evaluations** : Jobê³¼ Clientì˜ ìƒíƒœê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ìƒíƒœë¥¼ í‰ê°€í•˜ì—¬ Allocation(í• ë‹¹)ì„ ì¡°ì •í•´ì•¼í•˜ëŠ”ì§€ íŒë‹¨
* **Bin Packing** : ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‚¬ìš©ìœ¨ì„ ê·¹ëŒ€í™”
* **Datacenters** : ì¼ë°˜ì ìœ¼ë¡œ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì œê³µì‚¬ê°€ ê°€ìš©ì˜ì—­(AZ)ë¡œ ì •ì˜í•˜ëŠ” ë¬¼ë¦¬ì  ë˜ëŠ” ë…¼ë¦¬ì  ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ê·¸ë£¹
* **Regions** : ì—¬ëŸ¬ Datacenterë¡œ êµ¬ì„±ë  ìˆ˜ ìˆê³ , í•˜ë‚˜ì˜ Nomad í´ë¼ìŠ¤í„°ë¥¼ í¬í•¨í•˜ëŠ” Nomadì˜ ë…¼ë¦¬ì ì¸ êµ¬ì¡°
* **Federated** : ì—¬ëŸ¬ Regionì„ ì—°ë™
]

???
-  Allocations map tasks, task groups, and jobs to various client resources.
-  Allocations are adjusted based on Nomad Evaluations that are performed whenever balance within the system is disrupted, either through adjustments to the job, and/or changes to the client availability.
-  Nomad uses a highly efficient bin packing algorithm to ensure that resource utilization is maximized across the client cluster.
-  A cluster can reside within a traditional datacenter, or across multiple data centers defined as a Nomad region.
- Regions can be federated together enabling wider communication without replicating data across all regions.

---
name:  Deployment Architecture
# ì¼ë°˜ì ì¸ êµ¬ì„± ì•„í‚¤í…ì²˜

.smaller[
* Nomad ë°”ì´ë„ˆë¦¬ëŠ” ì„œë²„ì™€ í´ë¼ì´ì–¸íŠ¸ ê¸°ëŠ¥ì„ ëª¨ë‘ í¬í•¨
* ê° Nomad í´ëŸ¬ìŠ¤í„°ëŠ” 3~5ê°œì˜ ì„œë²„ë¡œ êµ¬ì„±
* ì„œë²„ëŠ” ìš°ì„ ìˆœìœ„(priorities), í‰ê°€(evalutions), í• ë‹¹(Allocations)ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ 'Leader'ë¥¼ ì„ ì¶œ
* í´ëŸ¬ìŠ¤í„°ëŠ” ë¬¼ë¦¬ì  ë°ì´í„°ì„¼í„°ë¥¼ ë„˜ì–´ êµ¬ì„± ê°€ëŠ¥
]
.center[![:scale 80%](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/NomadRegion.png)]

???
-  Nomad utilizes a single binary application that can be run as a client or server.
-  A server cluster should utilize 3-5 server nodes.
-  Servers communicate via the Gossip protocol, and use Consensus protocol to elect a leader
-  A single cluster of servers operate in a single region, which may consist of one or more data centers.

---
class: img-right
name:  Nomad Cluster Leader Election and Viability
# í´ëŸ¬ìŠ¤í„°ë‚´ì˜ "Leader" ì„ ì¶œ

![ServerElection](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/ServerElection.png)

.smaller[
* ì„œë²„ëŠ” ì„œë¡œê°€ í›„ë³´ë¡œì„œ ê²½ìŸ
    * ì„œë²„ 2ê°€ ë¨¼ì € ìì‹ ì„ ì•Œë¦¬ê³  íˆ¬í‘œ ìš”ì²­í•¨
* ì„œë²„ 1ì´ ì„œë²„ 2ë¥¼ ë¨¼ì € ì„ ì¶œ
    * ì„œë²„ 2ì— íˆ¬í‘œ
* ì„œë²„ 3ì€ íˆ¬í‘œì—ì„œ ì§€ê³  "Follower"ë¡œ ë³€ê²½ë˜ë©°, ì„œë²„ 2ê°€ "Leader"ë¡œ ì„ ì¶œ
]

.smaller.center[3 Servers can sustain 1 Failure, 5 servers can sustain 2 failures]


???
-  When servers initialize, they need to find eachother and create a leader.
-  A server will promote itself as a candidate to be a leader, and notify the other servers in the cluster.
-  Once the candidate has a quorum of votes, it will promote itself as the leader.
-  With 3 server nodes, the cluster can sustain a single failure.  With 5 server nodes, the cluster can sustain two failures.
-  Note that as you increase server members, it will take longer for the consensus protocol to converge and elect a leader.

---
name:  Multi-region Federation
# Region ê°„ ìš´ì˜

.smaller[
* ì—¬ëŸ¬ê°œì˜ Nomad Region(Cluster)ë¥¼ í•˜ë‚˜ë¡œ ì—°í•© ê°€ëŠ¥
* Jobì€ íŠ¹ì • Regionì—ì„œ ì œì¶œë˜ê³  ëŒ€ìƒì€ ì—¬ëŸ¬ Regionì¼ ìˆ˜ ìˆìŒ
* ACL í† í°, Policy, Sentinelì€ Regionê°„ ê³µìœ  ê°€ëŠ¥
* ì• í”Œë¦¬ì¼€ì´ì…˜/ìƒíƒœì— ëŒ€í•œ ë°ì´í„°ëŠ” ê³µìœ ë˜ì§€ ì•ŠìŒ

]

.center[![:scale 80%](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/Multi-Region.png)]

???
- Clusters can federate across regions using WAN Gossip
- Only ACL, Policies, and Sentinel Policies are shared across regions (no application data).

---
name:  Multi-region Federation
# Region ì„œë²„ ì¥ì•  ì‹œ

.smaller[
* íŠ¹ì • Regionì˜ ì„œë²„ì— ë¬¸ì œê°€ ìƒê¸°ë©´ í´ë¼ì´ì–¸íŠ¸ê°€ ì—°í•©ëœ íƒ€ Regionì— ì ‘ê·¼ ê°€ëŠ¥
* í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì„œë²„ì— ì ‘ì†ì´ ê°€ëŠ¥í•´ì•¼ í•¨
* ë‹¤ì¤‘ Regionì— ëŒ€í•œ RPC, Serf í†µì‹  í•„ìš”

]

.center[![:scale 80%](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/Failed-Region.png)]

???
-  í•œ ì§€ì—­ì˜ ì„œë²„ í´ëŸ¬ìŠ¤í„°ê°€ ì™„ì „íˆ ë‹¤ìš´ë˜ë©´ ë‹¤ë¥¸ ì§€ì—­ì˜ ì„œë²„ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ë¥¼ ìš©ì´í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
-  ì´ ë‹¤ì¤‘ ì§€ì—­ ì—°í•©ì—ëŠ” ì§€ì—­ ê°„ RPC ë° Serf ì§€ì›ì´ í•„ìš”í•©ë‹ˆë‹¤..

---
class: img-right
name:  Nomad Layout and Comms
# Nomad Communications
![NomadArchitectureRegion](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/nomad-architecture-region.png)

.smaller[
* 3~5 ê°œì˜ ì„œë²„ ë…¸ë“œ (í™€ìˆ˜, RAFT ê¸°ë°˜)
* LeaderëŠ” Followerì—ê²Œ ìƒíƒœë¥¼ ë³µì œ
* FollowerëŠ” í´ë¼ì´ì–¸íŠ¸ì˜ ë°ì´í„°ì™€ ìš”ì²­ì„ Leaderì—ê²Œ ì „ë‹¬
* ì„œë²„ëŠ” í´ë¼ì´ì–¸íŠ¸ì— í• ë‹¹ëœ ì‘ì—…ì„ ì „ë‹¬
* í´ë¼ì´ì–¸íŠ¸ëŠ” RPCë¥¼ í†µí•´ ëª¨ë“  ì„œë²„ì™€ í†µì‹ 

]

???
-  Within the Server Cluster, we have a Leader, and we have Followers.
-  Leaders are elected via quorum (which is why it is important to have 3-5 nodes) using Nomad's Consensus prootcol, based on RAFT.
-  Leader of the servers makes all allocation decisions, and distributes to Followers.
-  Server push allocation and task assignments via RPC to each Server.

---
name: Nomad Scheduler Section
class: title, shelf, no-footer, fullbleed
background-image: url(https://hashicorp.github.io/field-workshops-assets/assets/bkgs/HashiCorp-Title-bkg.jpeg)
count: false

# Nomad Scheduler Processes
## Evaluations, Allocations, Priorities, Preemption

![:scale 15%](https://hashicorp.github.io/field-workshops-assets/assets/logos/logo_nomad.png)

???
Focusing more on the Scheduler process

---
class: img-right
name:  Nomad Evaluation
# Nomad ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - Evaluations

![NomadEvalAlloc](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/Nomad_eval_alloc.png)

Evaluation(í‰ê°€)ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ë°œìƒí•  ë•Œë§ˆë‹¤ "ì‹œì‘"ë©ë‹ˆë‹¤.
.smaller[

* ìƒˆë¡œìš´ Jobì´ ìƒì„±
* Jobì´ ì—…ë°ì´íŠ¸ ë˜ê±°ë‚˜ ìˆ˜ì • ë¨
* Job ë˜ëŠ” Nodeê°€ ì‹¤íŒ¨

]

???
-  Evaluations to determine if any work is necessary.
-  Evaluation is initiated by a new job definition, an updated job definition, or some change to the infrastructure.
-  If necessary, a new Allocation maps tasks or task groups within jobs, to the available nodes

---
class: img-right

name:  Nomad Scheduler
# Nomad ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘

![NomadEvaluationKickoff](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/Nomad_Evaluation_Kickoff.png)

.smaller[
* Evaluationì´ ì‹œì‘ë˜ëŠ” ë°©ë²•ê³¼ ë¬´ê´€í•˜ê²Œ í‰ê°€ ìš”ì²­ëŠ” ëª¨ë“  ì„œë²„ ë…¸ë“œë¡œ ë³´ë‚´ì§‘ë‹ˆë‹¤.
* ëª¨ë“  í‰ê°€ì˜ ê²°ê³¼ëŠ” Leaderì˜ "Evaluation Broker"ì—ê²Œ ì „ë‹¬ë©ë‹ˆë‹¤.
* í‰ê°€ëŠ” Leaderê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ëŒ€ê¸° ì‹œí‚¬ ë•Œê¹Œì§€ "Pending" ìƒíƒœë¡œ ìœ ì§€ë©ë‹ˆë‹¤.

]

???
-  A new job, a modified or updated job, or any change in the system (job or node failure) will cause an evaluation to kick off.
-  Any of the server nodes can receive the evaluation request.
-  Evaluations are forwarded to a dedicated process on the Leader, called the evaluation broker.
-  Evaluation remains in 'pending' state until broker decides upon allocation

---
class: img-right

name:  Nomad Evaluation
# Nomad Evaluation(í‰ê°€)
![EvaluationQueue](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/Evaluation_Queue.png)

.smaller[

- "Evaluation Broker"ê°€ í‰ê°€ ëŒ€ìƒì„ ë°›ìœ¼ë©´ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë³€ê²½ ì‚¬í•­ì„ ëŒ€ê¸°ì—´ì— ë„£ìŠµë‹ˆë‹¤.

- Follower ì„œë²„ ë…¸ë“œì˜ ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ëŒ€ê¸°ì—´ì—ì„œ í‰ê°€í•­ëª©ì„ ì„ íƒí•˜ê³  "Plan"ì„ ì‹œì‘í•©ë‹ˆë‹¤.

- Leader ë…¸ë“œì— ìƒì£¼í•˜ëŠ” "Evaluation Broker"ëŠ” ë³´ë¥˜ì¤‘ì¸ í‰ê°€ ëŒ€ê¸°ì—´ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
- Jobì˜ ì •ì˜ì— ë”°ë¼ ìš°ì„  ìˆœìœ„ê°€ ê²°ì • ë©ë‹ˆë‹¤.

]

???
-  Here the evaluation Broker, residing on the leader node, manages the queue of pending evaluations.
-  Priority is determined based on Job definition
-  Broker ensures that somebody picks up the evaluation for processing.
-  Once the evaluation is picked up by a Scheduler, the planning begins!

---
name:  Scheduling Workers
# Scheduler Operations

ëª¨ë“  ì„œë²„ëŠ” ìŠ¤ì¼€ì¤„ë§ ì‘ì—…ìë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
* ê¸°ë³¸ì ìœ¼ë¡œ CPU ì½”ë“œ ë‹¹ í•˜ë‚˜ì˜ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ í•­ë‹¹
* ì„¸ê°€ì§€ ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
    * **Service** : ìˆ˜ëª…ì´ ê¸´(ì›¹ì„œë²„ ê°™ì€) ì„œë¹„ìŠ¤ì— ìµœì í™” ëœ ìŠ¤ì¼€ì¤„ëŸ¬
    * **Batch** : ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ë°°ì¹˜ ì‘ì—…ì„ ë¹ ë¥´ê²Œ ë°°ì¹˜
    * **System** : ëª¨ë“  ë…¸ë“œì—ì„œ ì‘ì—…ì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìŠ¤ì¼€ì¤„ëŸ¬

???
-  Each server node runs one scheduler per CPU core.
-  Server chooses the proper scheduler, either for standard services, batch jobs, or system level jobs.

---
name:  Scheduler Function Part 2
# Scheduler Processing
ìŠ¤ì¼€ì¤„ëŸ¬ê°€ Jobì„ ìˆ˜í–‰í–ˆë‹¤ëŠ” ê°€ì •í•˜ì— ê·¸ ê¸°ëŠ¥ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
.smaller[

1.  Jobì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤/ë…¸ë“œë¥¼ ì‹ë³„
2.  Bin packingì„ ê¸°ë°˜ ë˜ëŠ” ê¸°ì¡´ì˜ taskì™€ Jobì„ ê¸°ì¤€ìœ¼ë¡œ ë…¸ë“œì˜ ìˆœìœ„ë¥¼ ì§€ì •
3.  ìµœìƒì˜ ë…¸ë“œë¥¼ ì„ íƒí•˜ê³  Allocation(í• ë‹¹)ê³„íšì„ ìƒì„±
4.  Leaderì—ê²Œ í• ë‹¹ ê³„íš ì œì¶œ

]
???

-  Server process has several steps
-  First it identifies the potential nodes, or available resources, that could accept the job.
-  Next take a look at the ideal nodes, based on bin packing and existing tasks.
   -  Bin packing ensures the most efficient usage of the resources.
-  Taking existing tasks into account minimizes co-locating tasks on the same servers.
-  Highest ranking node is chosen, the allocation plan is created, and submitted back to the Leader.

---
class: img-right
name:  Plan Queue Processing
# Plan Queue Processing
![QueueProcessing](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/Queue_Processing.png)

ë‹¤ì‹œ Leaderë¡œ ëŒì•„ê°€ì„œ...
.smaller[

5.  ì œì¶œëœ ëª¨ë“  Allocation(í• ë‹¹) Planì„ í‰ê°€
6.  Planì„ ìˆ˜ë½, ê±°ë¶€ ë˜ëŠ” ë¶€ë¶„ ê±°ë¶€
7.  êµ¬ì„±, ì¼ì • ë³€ê²½ ë˜ëŠ” ì¢…ë£Œë¥¼ ìœ„í•´ ìŠ¤ì¼€ì¤„ëŸ¬ì— ì‘ë‹µì„ ë°˜í™˜
8.  ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” Evaluation(í‰ê°€)ì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸ í•˜ê³  Evaluation Brokerì™€ í™•ì¸
9.  í´ë¼ì´ì–¸íŠ¸ëŠ” í• ë‹¹ ë³€ê²½ ì‚¬í•­ì„ ì„ íƒí•˜ê³  ë™ì‘ ìˆ˜í–‰

]

???
-  Leader makes final determination for allocation.
-  All pending plans are prioritized and eliminate any concurrency if it exists.
-  Leader will either accept or reject (or partial reject) the plan.
-  Scheduler can chose to reschedule or terminate the request
-  Scheduler updates the Evaluation Broker with the decision, and clients pick up any changes deemed necessary

---
name:  End to End Flow
#  Flow Recap
![:scale 90%](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/Nomad_Overall_Flow.png)

---
name:  Job Priority
# Job Priority(ìš°ì„ ìˆœìœ„)
* ëª¨ë“  ìŠ¤ì¼€ì¤„ëŸ¬, í”Œë˜ë„ˆ, í”„ë¡œê·¸ë¨ ê´€ë¦¬ìëŠ” ê¹Œë‹¤ë¡œìš´ ì£¼ì œì¸  'ìš°ì„ ìˆœìœ„'ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
* Nomad ë˜í•œ Evaluation, Planning ê³¼ì •ì—ì„œ ìš°ì„ ìˆœìœ„ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
* ëª¨ë“  ì‘ì—…ì—ëŠ” ê´€ë ¨í•œ ìš°ì„ ìˆœìœ„ê°€ ìˆìŠµë‹ˆë‹¤.
* ìš°ì„ ìˆœìœ„ì˜ ë²”ìœ„ëŠ” 1~100 ì…ë‹ˆë‹¤.
* ë†’ì€ ìˆ«ìê°€ ë†’ì€ ìš°ì„ ìˆœìœ„ë¥¼ ê°–ìŠµë‹ˆë‹¤.

]

.center[ìš°ì„  ìˆœìœ„ê°€ ë” ë†’ì€ ì‘ì—…ì´ ì˜ˆì•½ë˜ë©´ ì–´ë–»ê²Œ ë™ì‘í• ê¹Œìš”?]


???
-  Nomad supports priority configuration with every Job, from 1 to 100.
-  The higher the number, the higher the priority.
-  What if a higher priority job is scheduled and resources are limited?

---
name:  Preemption
# Nomadì—ì„œì˜ Preemption(ì„ ì )
.center[Preemption(ì„ ì )ì„ í†µí•´ ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ì‘ì—…ì´ ë‹¤ë¥¸ ì‘ì—…ì„ ëŒ€ì²´ í•©ë‹ˆë‹¤.]
.smaller[

| Without Preemption            | With Preemption                  |
|-------------------------------|-------------------------------------------------|
|Jobê³¼ TaskëŠ” ìˆœì„œëŒ€ë¡œ í• ë‹¹ |ìì› ê°€ìš©ì„±ì— ê´€ê³„ì—†ì´ í‰ê°€ ìˆ˜í–‰ |
|ë¦¬ì†ŒìŠ¤ê°€ ê°€ìš©í•  ë•Œê¹Œì§€ í• ë‹¹ë˜ì§€ ì•Šì€ ë³´ë¥˜ ìƒíƒœ                   |í•„ìš”í•œ ê²½ìš° ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë‚®ì€ ì‘ì—…ì„ ì œê±°|
|             |'Plan'ì˜ ê²°ê³¼ëŠ” ëª¨ë“  ì‚¬ì „ ì„ ì ì„ ì‹ë³„|
]

.center[Preemption(ì„ ì )ì€ Namespaceë¡œ ë²”ìœ„ë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.]

???
-  Jobs are evaluated and allocated as they are delivered to the evaluation broker.
-  If resources aren't avialable, any evaluations will be stuck in pending state until resources become available.
-  With preemption, Nomad evicts lowest priority jobs if necessary.
-  Any preemption actions necessary are highlighted as an output of the 'Plan' operation
- Nomad Preemption is not scoped to Nomad namespaces. It operates at the cluster level. A high priority job targeted to one namespace can preempt lower-priority jobs in other namespaces.

---
class: col-2
name:  How Preemption Works
layout: false
# Preemption Details 1

![systemalert](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/SystemAlerting.png)
.center[System Alerting ì•±ì„ ì¶”ê°€í•˜ê³  ì‹¶ì§€ë§Œ ê³µê°„ì´ ì—†ìŠµë‹ˆë‹¤.]

![fullcluster](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/FullCluster.png)
.center[Nomad í´ëŸ¬ìŠ¤í„°ëŠ” ì´ë¯¸ 12GBì˜ ë©”ëª¨ë¦¬ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.]



???
* Let's run through a quick example of how preemption works.
* Here we have a Nomad cluster with a few allocations in an analytics solution.
* All allocations are all happy, and now we have a new job added to the system for System Alerting.
* We have enough CPU, and plenty of Hard Drive, but we are at the memory limit.
* There's no room at the inn for our System Alerting process.
* Without Preemption, that's where we would stop.
* But we have preemption, so we'll continue

---
class: col-2
name:  How Preemption Works 2
layout: false
# Preemption Details 2

![EvictBusinessAlert](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/EvictBusinessAlert1.png)
.center[Business Reporting ì•± ì¤‘ í•˜ë‚˜ì˜ ë¦¬ì†ŒìŠ¤ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.]

![AddSystemAlert](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/AddSystemAlert1.png)


???
With Preemption, Nomad realizes that there are lower priority allocations that can be evicted.  So if we are adding one System Alerting job, we evict one Business Reporting Job.  The Business Reporting job has the lowest priority, so it gets evicted first.  But what happens if we have to add two more System Alerting allocations?

---
class: col-2

name:  How Preemption Works 3
# Preemption Details 3

![EvictBusinessAlert](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/EvictBusinessAlert2.png)
.center[ë” ë§ì€ System Alerting ì€ ìš°ì„ ìˆœìœ„ê°€ ë‚®ì€ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë¥¼ í‡´ì¶œì‹œí‚µë‹ˆë‹¤.]
![AddSystemAlert](https://hashicorp.github.io/field-workshops-nomad/slides/multi-cloud/nomad-oss/images/AddSystemAlert2.png)
 .center[Log Collection ì„œë¹„ìŠ¤ëŠ” ìš°ì„ ìˆœìœ„ì˜ ì°¨ì´ê°€ 10 ë¯¸ë§Œì´ê¸°ì— ì„ ì  êµì²´ ëŒ€ìƒì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.]

???
* If we add two more Sytem Alerting allocations, we need to bump a Batch Analytics Allocation as well.
* Evicting the Log Collection allocation would be sufficient, but the Batch Analytics Allocation has a lower priority.
* Additionally, as the priority difference between System Alerting and Log Collection is less than 10, the Log Collection allocation isn't a candidate for preemption with respect to System Alerting.
---
name: Nomad within HashiCorp
class: title, shelf, no-footer, fullbleed
background-image: url(https://hashicorp.github.io/field-workshops-assets/assets/bkgs/HashiCorp-Title-bkg.jpeg)
count: false

# Nomad í†µí•©(Integration)
## The HashiCorp Ecosystem

![:scale 15%](https://hashicorp.github.io/field-workshops-assets/assets/logos/logo_nomad.png)

???
Nomad integrates well with other HashiCorp products.  We're just going to touch on the functionality here.

---
layout: true

.footer[
- Copyright Â© 2021 HashiCorp
- ![:scale 100%](https://hashicorp.github.io/field-workshops-assets/assets/logos/HashiCorp_Icon_Black.svg)
]

---
name:  Nomad and Consul
# Nomadì™€ Consulì˜ ê¸´ë°€í•œ í†µí•©

* Consulì€ ë‹¤ìŒì„ ì œê³µí•©ë‹ˆë‹¤. :
  * Nomad ì„œë²„ ë° í´ë¼ì´ì–¸íŠ¸ì˜ ìë™ í´ëŸ¬ìŠ¤í„°ë§
  * Jobì— ëŒ€í•œ ì„œë¹„ìŠ¤ íƒìƒ‰
  * Jobì— ì˜í•´ ì‹¤í–‰ë˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•œ ë™ì ì¸ êµ¬ì„±
* Consul Connect ëŠ” ë‹¤ìŒì„ ì œê³µí•©ë‹ˆë‹¤. :
  * Jobê³¼  Task groupsìœ¼ë¡œ ë°°í¬ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë³´ì•ˆ í†µì‹ 

???
-  Nomad Servers and Clients can automatically find each other within the network, minimizing configuration and being more address-flexible.
-  Consul enables application service nodes to be automatically discoverable within the cluster
-  Configuration files can be dynamically created utilizing environment variables or even Vault secrets with templating
-  Consul Connect can secure communication between services deployed in public or private clouds.

---
name:  Nomad and Vault
# Nomadì™€ Vaultì˜ í†µí•©

* Nomadê°€ Vaultì˜ í† í°ì„ ìƒì„±í•˜ê³  Jobì— ë°°í¬í•©ë‹ˆë‹¤.
* Vaultë¡œ ë¶€í„° ì‹œí¬ë¦¿ ë°ì´í„°(e.g. password)ë¥¼ ê²€ìƒ‰í•˜ê³  í• ë‹¹í•©ë‹ˆë‹¤.
* Jobì€ Vaultì—ì„œ ë™ì ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ë‹¨ê¸° ìê²© ì¦ëª…ìœ¼ë¡œ ëŒ€ìƒ ì„œë¹„ìŠ¤(e.g. DB)ì— ì ‘ê·¼í•©ë‹ˆë‹¤.
* TaskëŠ” Vaultì˜ Nomad Secret Engineì„ ì‚¬ìš©í•˜ì—¬ Nomad API í† í°ì„ í™•ì¸í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
* Nomad Enterpriseì—ì„œëŠ” Vaultì˜ Namespaceì™€ ì—°ê³„ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

???
-  Nomad's integration with Vault allows Vault tokens to be used by Nomad Tasks
-  Nomad's tasks can retrieve secrets directly from Vault
-  Vault can also provide short-lived credentials to Nomad tasks
-  Vault offers a native Nomad Secrets Engine
-  Nomad Enterprise supports integrating a single Nomad cluster with a Vault cluster that has multiple namespaces.

---
name: chapter-2-summary
# ğŸ“ Chapter 2 Summary
In this chapter, you learned about:
  * Nomadì˜ ì£¼ìš” ì»¨ì…‰
  * Nomad's Architecture
  * Nomad's Scheduling Processes
  * Nomad í†µí•©(Integration) with Consul and Vault
